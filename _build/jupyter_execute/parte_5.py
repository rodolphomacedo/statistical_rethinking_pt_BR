#!/usr/bin/env python
# coding: utf-8

# # 5 - Muitas Vari√°veis e os Waffles Esp√∫rios

# Nosso objetivo nessa parte √© come√ßar o processo da constru√ß√£o de modelos de regress√µes m√∫ltiplas e, assim, tamb√©m come√ßaremos a criar as bases para o `framework de infer√™ncias causais`.
# 
# Para iniciarmos a discuss√£o iremos introduzir um exemplo emp√≠rico, ou seja, um exemplo baseado na experi√™ncia e observa√ß√µes, sejam elas baseadas em algum m√©todo (*met√≥dicas*) ou n√£o.

# <img src="./images/waffle_house.jpeg" alt="Waffle House" width=1000 />
# 
# [Fonte](https://br.linkedin.com/company/waffle-house)

# 
# [Aula - Statistical Rethinking Winter 2019 Lecture 05](https://www.youtube.com/watch?v=e0tO64mtYMU)

# A *waffle house* √© uma cadeia de restaurantes com mais de $2000$ locais em $25$ estados nos EUA (mapa amarelo abaixo). A maioria das suas localiza√ß√µes est√° no Sul do pa√≠s e √© um item da cultural e regional norte americana. (*[wikipedia](https://en.wikipedia.org/wiki/Waffle_House)*). Uma das particularidades dessa rede de restaurantes √© que eles trabalham 24 horas. Nunca fecham! Essa √© a proposta de neg√≥cio deles.
# 
# Um outro fato importante e desagrad√°vel que se surge no sul dos EUA s√£o os *Furac√µes*. Esses s√£o causados pelas depress√µes tropicais clim√°ticas do pais. Os restaurantes da rede Waffles s√£o um dos √∫nicos estabelecimentos que continuam abertos nesses per√≠odos turbulentos. *Exceto quando esses furac√µes atingem uma de suas lojas.*
# 
# A rede √© t√£o confi√°vel que governadores dos EUA criaram o `waffle house index`, internamente na FEMA (*ag√™ncia federal de gest√£o de emerg√™ncias*). Usada como uma m√©trica informal com o nome da rede de restaurantes, pretendem  determinar o efeito das tempestades, como uma escala aux√≠liar para o planejamento de recupera√ß√£o de um desastre. (*[waffle house index](https://en.wikipedia.org/wiki/Waffle_House_Index)*)

# <img src="./images/WH_per_people.png" alt="waffle house map" width=900 />
# 
# Imagem - https://www.scottareynhout.com/blog/2017/10/7/waffle-house-map

# Al√©m dos desastres naturais, existem tamb√©m muitas outras coisas acontecendo em grande escala no sul dos EUA, tal como **div√≥rcios**!
# 
# No gr√°fico abaixo temos a indica√ß√£o da quantidade de div√≥rcios nos EUA. Observe o sul do pa√≠s e compare com o mapa acima.

# <img src="./images/WH_per_divorce.png" alt="Waffle House contry-level-marriage-divorce-data-2010" width=900 />
# 
# Imagem - https://www.bgsu.edu/ncfmr/resources/data/original-data/county-level-marriage-divorce-data-2010.html

# Percebeu? 
# 
# Em ambos os mapas existem uma grande concentra√ß√£o, no extremo do sul do mapa, de restaurantes da rede Waffle House e, subindo mais ao norte do pa√≠s, temos quantidades cada vez menores. O mesmo ocorre no mapa das *taxas de div√≥rcios*, quando olhamos para os mesmos locais no mapa.
# 
# Podemos ent√£o fazer uma estimativa: `esses dados est√£o correlacionados entre si`. E podemos nos ser levados a pensar que quanto maior a concentra√ß√£o de restaurantes na regi√£o maior seria seria a taxa de div√≥rcios.
# 
# E por que isso acontece? 
# 
# Pelo seguinte motivo: `Por nada`! 
# 
# Isso mesmo, nada!!!
# 
# N√£o existe nada que tenha uma rela√ß√£o direta na qual a quantidade de restaurantes da rede em alguma determinada regi√£o influencie casais a brigarem e tomarem a decis√£o de se separar! 
# 
# √â estranho. √â c√¥mico. √â intrigante. Isso √© uma `Correla√ß√£o Esp√∫ria`!

# # Correla√ß√µes Esp√∫rias
# 
# Essas s√£o as `correla√ß√µes esp√∫rias`, ou seja, `correla√ß√µes sem certeza`; que n√£o √© verdadeira nem real; √© hipot√©tica!
# 
# Muita coisa est√° relacionada com as outras no mundo real. `Isso √© a Natureza`!
# 
# Por exemplo, se quisermos, por qualquer motivo que seja, arrumar um "argumento" para enfraquecer a imagem da rede Waffle House, "podemos" usar essas correla√ß√µes esp√∫rias com um dos argumentos. Assim, n√≥s iriamos expor na m√≠dia a seguinte manchete: 
# 
# ```{admonition} Breaking News:
# Pesquisadores [da universidade xyz] indicam que: o aumento do n√∫mero de restaurantes da rede Waffle House 
# na regi√£o implica num aumento assustador no n√∫mero de div√≥rcios nessa mesma regi√£o!
# ```
# 
# 
# Isso soa estranho, eu sei! Esse √© apenas um exemplo extremo.
# 
# Mas, l√° no fundo, esse tipo de pensamento n√£o soa t√£o estranho no dia-a-dia...

# Existem diversas correla√ß√µes esp√∫rias no mundo. Muita coisa tem correla√ß√£o com muitas outras coisas.
# 
# 
# ```{admonition} Entretanto:
# Essas correla√ß√µes n√£o implicam causalidade.
# ```

# Mas para entendermos melhor, vamos ver mais alguns exemplos sobre essas correla√ß√µes esp√∫rias:
# 
# 
# - O consumo de queijo tem uma correla√ß√£o de $94.7\%$ com os acidentes fatais com o emaranhado do len√ßol de cama. 
# 
# 
# - O consumo per capita de frango apresenta uma correla√ß√£o de $89\%$ com a importa√ß√£o de petr√≥leo.
# 
# 
# - Os acidentes por afogamentos em piscina tem a correla√ß√£o de $66\%$ com o n√∫mero de filmes lan√ßados pelo Nicolas Cage, por ano. Veja graficamente essa correla√ß√£o abaixo:
# 
# 
# <img src="./images/chart.jpeg" alt="Tyler Vigen spurious correlations" width=1000>
# 
# 
# 
# 
# Percebeu?
# 
# Se o consumo de frango diminu√≠sse, a importa√ß√£o provavelmente n√£o sofreria nenhum impacto por essa causa. E, caso o consumo de queijo diminuir, tamb√©m n√£o haver√° uma diminui√ß√£o nos acidentes fatais das pessoas que est√£o dormindo em suas camas. E, como √© esperado, se o Nicolas Cage se aposentar dos cinemas, os acidentes por afogamento continuar√£o constantes. 
# 
# ```{tip}
# Correla√ß√£o n√£o implica causalidade!
# ```
# 
# 
# 
# Por fim, ter mais lojas da rede Waffle House n√£o `causa` mais div√≥rcios na regi√£o.
# 
# 
# 
# ----
# Mais correla√ß√µes esp√∫rias, tais como essas acima, podem ser encontradas no site do [Tyler Vigen](https://www.tylervigen.com/spurious-correlations).
# 
# 
# -----
# 
# Entendido essa parte, vamos ao objetivo desse cap√≠tulo.
# 

# # Regress√£o M√∫ltiplas
# 
# Vamos ver como construir um modelo de regress√£o linear novamente. Mas dessa vez iremos ver tamb√©m como se faz com  `m√∫ltiplas vari√°veis` e quais s√£o suas implica√ß√µes. 
# 
# ## Pr√≥s e contras das m√∫ltiplas vari√°veis:
# 
# - A parte boa desse tipo de modelo √© que as regress√µes m√∫ltiplas podem n√£o s√≥ revelar correla√ß√µes esp√∫rias como tamb√©m podem revelar associa√ß√µes escondidas que n√≥s n√£o far√≠amos normalmente, ou seja, n√£o ter√≠amos visto essas associa√ß√µes usando o modelo com uma simples vari√°vel preditora.
# 
# 
# - Mas, por outro lado, podemos tamb√©m adicionar vari√°veis explicativas aos modelos que contenham correla√ß√µes esp√∫rias √† regress√£o m√∫ltipla e, podem tamb√©m, esconder as algumas das reais associa√ß√µes que existem.
# 
# 
# Ent√£o, como essas coisas geralmente n√£o s√£o bem explicadas, vamos detalhar todo o processo de constru√ß√£o de uma regress√£o m√∫ltipla, a seguir.
# 
# Quando constru√≠mos um modelo usando uma regress√£o m√∫ltipla, n√≥s devemos ter uma estrutura mais profusa para pensar sobre as nossas decis√µes. Apenas *jogar* todas as vari√°veis explicativas dentro da regress√£o m√∫ltipla, como usualmente √© feito, √© o segredo para o fracasso da an√°lise e n√≥s n√£o queremos fazer isso!
# 
# Para isso precisamos de uma estrutura mais ampla e rica para conseguirmos pensar e tomar melhores decis√µes. Essa estrutura √© o `framework de infer√™ncia casual`.
# 
# Nessa parte iremos aprender do b√°sico sobre infer√™ncia casual:
# 
# - Grafos ac√≠clicos direcionados (DAGs - *Directed acyclic graphs*)
# 
# 
# - Fork, pipes, colliders...
# 
# 
# - Crit√©rio de Backdoor.

# J√° sabemos que o Waffle House n√£o causam os div√≥rcios. Mas o que causa os div√≥rcios?
# 
# J√° vimos no mapa acima que div√≥rcios do sul do pa√≠s tem uma taxa bem mais alta do que no restante mais ao norte. Existem muitos esfor√ßos para tentar identificar as causas e as taxas de div√≥rcios. Sabemos que no sul tem uma predomin√¢ncia religiosa quando comparada ao restante do pa√≠s. Isso deixa os cientistas com certas desconfian√ßas.

# Existem assim muitas coisas que est√£o correlacionadas com a taxa de div√≥rcios. Uma delas √© a `taxa de casamentos`. Tamb√©m podemos usar essas informa√ß√µes para cada um dos outros $50$ estados do pa√≠s. Todos eles t√™m uma `correla√ß√£o positiva` da taxa de casamentos com a taxa de div√≥rcios.
# 
# Um ponto importante nessa correla√ß√£o √© que *s√≥ pode acontecer um div√≥rcio se houver o casamento*! 
# 
# Mas a correla√ß√£o entre essas taxas podem ser tamb√©m `correla√ß√µes esp√∫rias`. Assim como a correla√ß√£o entre homic√≠dos e div√≥rcios s√£o uma correla√ß√£o esp√∫ria.
# 
# Pois uma taxa alta de casamentos pode indicar que a sociedade v√™ o casamento de modo favor√°vel e isso pode significar taxas de div√≥rcios mais baixas. N√£o necessariamente faz sentido, mas pode ser que as taxas de casamentos e de div√≥rcios sejam correla√ß√µes esp√∫rias tamb√©m. 
# 
# ```{note}
# - Correla√ß√£o n√£o implica Causalidade
# 
# 
# - Causalidade n√£o implica Correla√ß√£o
# 
# 
# - Causalidade implica em correla√ß√£o condicional
# ```
# 
# 
# Assim, precisamos mais do que apenas simples modelos! √â necess√°rio um estrutura mais robusta.
# 
# 
# Mas, o que causa os div√≥rcios? 
# 
# Vamos descobrir isso...

# Existe outra vari√°vel que tamb√©m √© correlacionada com a vari√°vel `taxa de div√≥rcio`, √© a vari√°vel `idade mediana das pessoas que se casam, em cada estado`. Mas diferente da `taxa de casamento`, essa vari√°vel apresenta uma *correla√ß√£o negativa*.

# In[1]:


import numpy as np
import pandas as pd
import stan
import nest_asyncio
import matplotlib.pyplot as plt
plt.style.use('default')


# In[2]:


# Definindo o plano de fundo cinza claro para todos os gr√°ficos feitos no matplotlib
plt.rcParams['axes.facecolor'] = 'lightgray'


# In[3]:


# Desbloqueio do asyncIO do jupyter
nest_asyncio.apply()


# In[4]:


def HPDI(posterior_samples, credible_mass):
    
    # Calcula o maior intervalo de probabilidades a partir de uma amostra
    
    # Fonte: https://stackoverflow.com/questions/22284502/highest-posterior-density-region-and-central-credible-region
    sorted_points = sorted(posterior_samples)
    ciIdxInc = np.ceil(credible_mass * len(sorted_points)).astype('int')
    nCIs = len(sorted_points) - ciIdxInc
    ciWidth = [0]*nCIs
    
    for i in range(0, nCIs):
        ciWidth[i] = sorted_points[i + ciIdxInc] - sorted_points[i]
        HDImin = sorted_points[ciWidth.index(min(ciWidth))]
        HDImax = sorted_points[ciWidth.index(min(ciWidth))+ciIdxInc]

    return(HDImin, HDImax)


# In[5]:


# ====================================
#   Lendo os dados da Waffle House
# ====================================

df = pd.read_csv('./data/WaffleDivorce.csv', sep=';')
df.head()


# In[6]:


# ============================================
#  Plotando os dados das taxas de div√≥rcios
# ============================================
fig, [ax1, ax2] = plt.subplots(1, 2, figsize=(17, 9))

plt.suptitle('Associa√ß√µes Esp√∫rias?')

ax1.scatter(df.Marriage.values ,df.Divorce.values)
ax1.grid(ls='--', color='white', linewidth=0.4)
ax1.set_xlabel('Taxa de Casamento')
ax1.set_ylabel('Taxa de Div√≥rcio')

ax2.scatter(df.MedianAgeMarriage.values ,df.Divorce.values)
ax2.grid(ls='--', color='white', linewidth=0.4)
ax2.set_xlabel('Mediana da Idade dos Noivos')
ax2.set_ylabel('Taxa de Div√≥rcio')

plt.show()


# Vamos construir modelos lineares simples para os dois gr√°ficos acima.

# In[7]:


# ========================================================
#  Construindo um modelo linear simples:
#
#    taxa de div√≥rcio ~ alpha + beta * taxa_casamento
# ========================================================
divorce_model1 = """
    data {
        int N;
        vector[N] divorce_rate;
        vector[N] marriage_rate;
    }
    
    parameters {
        real alpha;
        real beta;
        real<lower=0, upper=50> sigma;
    }
    
    model {
        divorce_rate ~ normal(alpha + beta * marriage_rate, sigma);
    }
"""

my_data = {
    'N': len(df.Divorce),
    'marriage_rate': df.Marriage.values,
    'divorce_rate': df.Divorce.values,
}

posteriori1 = stan.build(divorce_model1, data=my_data)
fit1 = posteriori1.sample(num_chains=4, num_samples=1000)

alpha_1 = fit1['alpha'].flatten()
beta_1 = fit1['beta'].flatten()
sigma_1 = fit1['sigma'].flatten()


# In[8]:


# ==============================================================
# Construindo um modelo linear simples:
#
#  taxa divorcio ~ alpha + beta * mediana da idade dos noivos
# ==============================================================

stan_model2 = """
    data {
        int N;
        vector[N] divorce_rate;
        vector[N] median_age_marriage;
    }
    
    parameters {
        real alpha;
        real beta;
        real<lower=0, upper=50> sigma;
    }
    
    model {
        divorce_rate ~ normal(alpha + beta * median_age_marriage, sigma);
    }
"""

my_data = {
    'N': len(df.Divorce),
    'divorce_rate': df.Divorce.values,
    'median_age_marriage': df.MedianAgeMarriage.values,
}

posteriori2 = stan.build(stan_model2, data=my_data)
fit2 = posteriori2.sample(num_chains=4, num_samples=1000)

alpha_2 = fit2['alpha'].flatten()
beta_2 = fit2['beta'].flatten()
sigma_2 = fit2['sigma'].flatten()


# In[9]:


# ====================================================================
#  Plotando os dados das taxas de div√≥rcios e suas estimativas m√©dias
# ====================================================================
fig, [ax1, ax2] = plt.subplots(1, 2, figsize=(17, 9))

plt.suptitle('Associa√ß√µes Esp√∫rias?')

# Calculando as regi√µes de HPDI para mu - Taxa de casamentos com 0.92 de credibilidade
range_mu = np.sort(df.Marriage)
posteriori_mu_aux = np.array([[alpha_1 + beta_1 * marriage for marriage in range_mu]])[0]
posteriori_mu_HPDI = np.array([[HPDI(posteriori_marriage, 0.92) for posteriori_marriage in posteriori_mu_aux]])[0]

# Gr√°fico de Div√≥rcio x Casamento
ax1.scatter(df.Marriage.values ,df.Divorce.values)
ax1.fill_between(range_mu, 
                 posteriori_mu_HPDI[:, 0], posteriori_mu_HPDI[:, 1], 
                 color='gray', alpha=0.4)
ax1.plot(df.Marriage.values, 
         alpha_1.mean() + beta_1.mean() * df.Marriage.values,
         color='black')

ax1.grid(ls='--', color='white', linewidth=0.4)
ax1.set_xlabel('Taxa de Casamento')
ax1.set_ylabel('Taxa de Div√≥rcio')
ax1.set_title('Correla√ß√£o Positiva \n Esp√∫ria?')

# ------------

# Calculando as regi√µes de HPDI para mu - Mediana para idade de casamentos com 0.92 de credibilidade
range_mu = np.sort(df.MedianAgeMarriage)
posteriori_mu_aux = np.array([[alpha_2 + beta_2 * mediaAgeMarriage for mediaAgeMarriage in range_mu]])[0]
posteriori_mu_HPDI = np.array([[HPDI(posteriori_mediaAgeMarriage, 0.92) for posteriori_mediaAgeMarriage in posteriori_mu_aux]])[0]

# Gr√°fico de Div√≥rcio x Mediana da Idade dos Casamentos
ax2.scatter(df.MedianAgeMarriage.values ,df.Divorce.values)
ax2.fill_between(range_mu, 
                 posteriori_mu_HPDI[:, 0], posteriori_mu_HPDI[:, 1], 
                 color='gray', alpha=0.4)

ax2.plot(df.MedianAgeMarriage.values, 
         alpha_2.mean() + beta_2.mean() * df.MedianAgeMarriage.values,
         color='black')

ax2.grid(ls='--', color='white', linewidth=0.4)
ax2.set_xlabel('Mediana da Idade dos Noivos')
ax2.set_ylabel('Taxa de Div√≥rcio')
ax2.set_title('Correla√ß√£o Negativa \n Esp√∫ria?')

plt.show()


# Observando os dois gr√°ficos acima, quais dessas correla√ß√µes s√£o `causas plaus√≠veis`? 
# 
# No gr√°fico da esquerda, sabemos que a `taxa de casamento` tem uma associa√ß√£o direta com a `taxa de div√≥rcio`, pois, para se divorciar √© necess√°rio antes ter casado. Mas essa interpreta√ß√£o dos fatos pode estar contaminada pelo nosso senso comum e, assim, uma associa√ß√£o esp√∫ria, nos seria v√°lida. Um dos motivos pra isso acontecer √© que em locais que possuem maiores incentivos religiosos e podem implicar em maiores incentivos para casamentos, podendo, assim, n√£o haver uma causalidade direta com o div√≥rcio.
# 
# J√° no gr√°fico √† direita, a `taxa de div√≥rcio` est√° associada com `mediana das idades dos noivos`, nos apresentando uma associa√ß√£o negativa. Isso pode nos levar a pensar que jovens, ao se casarem, tomam decis√µes muito emotivas e que isso tamb√©m levaria a altas taxas de div√≥rcios. Ou pode ser que essa seja uma associa√ß√£o esp√∫ria e nossas hip√≥teses e nosso senso comum n√£o tenha sentido.

# Assim, mesmo que nossas justificativas sejam bastante `veross√≠meis` a respeito do comportamento da Natureza do evento, n√£o podemos afirmar que essas associa√ß√µes n√£o sejam esp√∫rias.
# 
# Se a identifica√ß√£o de associa√ß√µes esp√∫rias, como no exemplo de filmes do *Nicolas Cage*, `pareciam √≥bvias` de serem identificadas, agora, essas n√£o parecem ser t√£o simples de perceber sua veracidade.
# 
# Portanto, √© necess√°rio construirmos um ferramental que nos permita identificar com mais facilidade e, com um maior grau de certeza, se essas associa√ß√µes s√£o reais ou s√£o esp√∫rias.
# 
# O que queremos fazer agora √© colocar essas duas vari√°veis no mesmo modelo e entender o que isso faz e por que nos revela, quase certamente, que determinada vari√°vel √© uma `impostora`.

# ## M√∫ltiplas causas de div√≥rcios

# <img src="./images/cidade1.jpg" alt="Aprender a taxa casamentos de uma cidade" />
# 
# [Fonte: *Hong Kong - wikipedia*](https://pt.wikipedia.org/wiki/Cidade#/media/Ficheiro:Hong_Kong_Night_Skyline.jpg)
# 
# Como entender o mecanismo das Causas Naturais de div√≥rcio numa cidade grande

# O que n√≥s queremos saber √© qual o valor de uma `vari√°vel preditora`, uma vez que n√≥s conhecemos os valores das outras vari√°veis preditoras corretamente?
# 
# Todas vari√°veis preditoras s√£o ,em alguma extens√£o, correlacionadas com as outras vari√°veis preditoras e, tamb√©m, com a vari√°vel resposta. Entretanto, elas t√™m `correla√ß√µes parciais` que s√£o reveladoras de informa√ß√µes adicionais dessa estrutura de correla√ß√£o.

# Para entendermos melhor, vamos exemplificar com nosso exemplo sobre div√≥rcios.
# 
# N√≥s gostariamos de aprender mais sobre o valor da `taxa de casamentos`, uma vez que j√° conhecemos a `mediana da idade da taxa dos noivos`.

# ## Entendendo os grafos um pouco mais perto.

# DAG's s√£o o acr√¥nimo para a express√£o ingl√™s `Directed Acyclic Graph`, o que √© portug√™s √© conhecido como `Grafos Ac√≠clicos Direcionados` [(*ver mais em wikipedia*)](https://pt.wikipedia.org/wiki/Grafos_ac%C3%ADclicos_dirigidos).

# In[10]:


# =======================================
#   Construindo o desenho de um DAG 
# =======================================

# -----------------------------------------------------
# Obs: Os pr√≥ximos grafos irei esconder os c√≥digos, 
#      para ficar visualmente melhor. Mas, se quiser,
#      os c√≥digos podem ser encontrados no github.
# -----------------------------------------------------

plt.figure(figsize=(17, 7))

plt.xlim(0, 1)
plt.ylim(0, 1)

size=40

plt.annotate('A', (0.2, 0.8), fontsize=size)
plt.annotate('M', (0.8, 0.8), fontsize=size)
plt.annotate('D', (0.495, 0.2), fontsize=size)

plt.title('Grafo dos Div√≥rcios')

# Edge: M <---> D
plt.annotate("", 
             xytext=(0.79, 0.79), xy=(0.53, 0.265) ,
             arrowprops=dict(arrowstyle="-|>,  head_width=1, head_length=1.2"))

# Edge: A <---> M
plt.annotate("", 
             xytext=(0.24, 0.82), xy=(0.77, 0.82),
             arrowprops=dict(arrowstyle="-|>,  head_width=1, head_length=1.2"))

# Edge: A <---> D
plt.annotate("", 
             xytext=(0.24, 0.77), xy=(0.48, 0.26),
             arrowprops=dict(arrowstyle="-|>,  head_width=1, head_length=1.2"))


# Remover os valores dos eixos
plt.yticks([])
plt.xticks([])

plt.show()


# DAG's s√£o ferramentas heur√≠sticas usadas no framework de infer√™ncias causais. Elas n√£o como ferramentas mec√¢nicas anal√≠ticas que j√° vimos, mas s√£o *incrivelmente* √∫teis para nos auxiliar a pensar melhor e eventualmente tamb√©m nos permite entender muito melhor os modelos mec√¢nicos.
# 
# As setas do *DAG* apontam apenas em uma `dire√ß√£o` e essa dire√ß√µes indica uma `rela√ß√£o de causalidade` entre as vari√°veis analisadas, ou seja, indica que uma vari√°vel tem `influ√™ncia direta` sobre a outra.
# 
# A aus√™ncia de *ciclos* significa que n√£o existem *loops* na causalidade. Mas esses loops podem acontecer sobre o tempo, o que tornaria a an√°lise tamb√©m uma `s√©rie temporal`!

# 
# A representa√ß√£o desses problemas reais em estruturas como *DAG's* podem ser, realmente, muito grandes. Pois, assim, podemos descrever a estrutura no tempo $T$, $\{T_1, T_2, ...\}$, e assim por diante.
# 
# N√≥s, normalmente rotulamos os grafos por dois nomes: `n√≥s` (*nodes*) e `arestas` (*edge*). (Iremos aqui, por conve√ß√£o, usar os nomes em ingl√™s.)
# 
# ```{note}
# Os nodes (n√≥s) s√£o as vari√°veis. J√° os edges (as arestas) representam a rela√ß√£o causal entre os nodes.
# ```

#  As associa√ß√µes que foram levadas em considera√ß√£o, podem  ser inclu√≠das dentro dos modelos de redes bayesianas, que s√£o considerados parte do conjunto de ferramentas de *Machine Learning*. Esses modelos tamb√©m n√£o levam em considera√ß√£o a causalidade, pois n√£o existe um mecanismo interno nesses modelos que leve em considera√ß√£o a dire√ß√£o da causalidade.
# 
# J√° nos *DAG's*, tal mecanismo, existe!
# 
# ```{note}
#  Esse mecanismo nos permite a capacidade de enxergar, atrav√©s das lentes probabil√≠sticas, a influ√™ncia da causa e efeito em diferentes eventos! Isso n√£o √© medir uma associa√ß√£o. √â uma medida para a causalidade! 
# ```
# 
# E isso, faz `toda` a diferen√ßa! 
# 
# 
# *Tamb√©m veremos qual √© a diferen√ßa entre as duas abordagens comnetadas...*

# ## Bons Grafos

# Queremos saber a diferen√ßa entre esses dois grafos abaixo. O da esquerda tem um caminho direto de $A <-> M <-> D$ e, no outro gr√°fico, uma suposi√ß√£o de que temos que a rela√ß√£o causal entre a taxa de casamento ($M$) e a taxa de div√≥rcio ($D$) n√£o existe mais.

# In[11]:


# =======================================
#   Construindo o desenho de um DAG 
# =======================================

# Esses c√≥digos devem estar invis√≠veis no jupyter-book 

plt.figure(figsize=(17, 7))

plt.xlim(0, 2)
plt.ylim(0, 1)

size=40

#-------------------------
# DAG - Esquerda
#-------------------------

plt.annotate('A', (0.2, 0.8), fontsize=size)
plt.annotate('M', (0.8, 0.8), fontsize=size)
plt.annotate('D', (0.48, 0.17), fontsize=size)

plt.title('Grafo dos Div√≥rcios')

# Edge: M <---> D
plt.annotate("", 
             xytext=(0.79, 0.79), xy=(0.53, 0.265) ,
             arrowprops=dict(arrowstyle="-|>,  head_width=1, head_length=1.2"))

# Edge: A <---> M
plt.annotate("", 
             xytext=(0.27, 0.82), xy=(0.77, 0.82),
             arrowprops=dict(arrowstyle="-|>,  head_width=1, head_length=1.2"))

# Edge: A <---> D
plt.annotate("", 
             xytext=(0.24, 0.77), xy=(0.48, 0.26),
             arrowprops=dict(arrowstyle="-|>,  head_width=1, head_length=1.2"))


#-------------------------
# DAG - Direita
#-------------------------

plt.annotate('A', (1.2, 0.8), fontsize=size)
plt.annotate('M', (1.8, 0.8), fontsize=size)
plt.annotate('D', (1.48, 0.17), fontsize=size)

plt.title('Grafo dos Div√≥rcios')

# Edge: M <---> D
#lt.annotate("", 
#             xytext=(1.79, 0.79), xy=(1.53, 0.265) ,
#             arrowprops=dict(arrowstyle="-|>,  head_width=1, head_length=1.2"))

# Edge: A <---> M
plt.annotate("", 
             xytext=(1.27, 0.82), xy=(1.77, 0.82),
             arrowprops=dict(arrowstyle="-|>,  head_width=1, head_length=1.2"))

# Edge: A <---> D
plt.annotate("", 
             xytext=(1.24, 0.77), xy=(1.48, 0.26),
             arrowprops=dict(arrowstyle="-|>,  head_width=1, head_length=1.2"))

# Remover os valores dos eixos
plt.yticks([])
plt.xticks([])

plt.show()


# Regress√µes Lineares, em princ√≠pio, podem nos dizer a diferen√ßa entre essas duas coisas. Mas uma regress√£o bivariada j√° n√£o pode mais. Elas podem nos dar apenas algum conhecimento da associa√ß√£o entre *casamentos* e *div√≥rcios*, mas `n√£o podem nos dizer qual a diferen√ßa entre esses dois *DAG's*` acima. 
# 
# Mas, porque n√£o? 
# 
# Por que, A tem influ√™ncia tanto na taxa de casamentos ($A <-> M$) quanto em D ($A <-> D$), isso gera uma `correla√ß√£o` entre os eventos *taxa de casamento* ($M$) e *taxa de div√≥rcio* ($D$), mesmo que uma n√£o influencie na outra, como no gr√°fico √† direita. (*Elegante essa explica√ß√£o, hein!*)
# 
# √â um modo bonito de dizer:
# 
# ```{warning}
# Correla√ß√£o n√£o √© Casualidade!
# ```
# 
# Essa mesmo estrutura do *DAG's*, pode ser modelada com as correla√ß√µes esp√∫rias do in√≠cio do cap√≠tulo, *waffle House* vs *taxa de div√≥rcios*

# In[12]:


# =======================================
#   Construindo o desenho de um DAG 
# =======================================

# Esses c√≥digos devem estar invis√≠veis no jupyter-book 

plt.figure(figsize=(17, 7))

plt.xlim(0, 2)
plt.ylim(0, 1)

size=20


plt.title('Grafo dos Div√≥rcios vs Waffle Houses')


#-------------------------
# DAG - Direita
#-------------------------

plt.annotate('?', (0.2, 0.8), fontsize=size+10)
plt.annotate('Quantidade de Lojas da Waffle House por regi√£o', (0.8, 0.8), fontsize=size)
plt.annotate('Taxa de Div√≥rcio por regi√£o', (0.48, 0.17), fontsize=size)


# Edge: ? <---> Haffle House
plt.annotate("", 
             xytext=(0.27, 0.82), xy=(0.77, 0.82),
             arrowprops=dict(arrowstyle="-|>,  head_width=1, head_length=1.2"))

# Edge: ? <---> Div√≥rcio
plt.annotate("", 
             xytext=(0.24, 0.77), xy=(0.48, 0.26),
             arrowprops=dict(arrowstyle="-|>,  head_width=1, head_length=1.2"))

# Remover os valores dos eixos
plt.yticks([])
plt.xticks([])

plt.show()


# A correla√ß√£o que existe entre essas duas vari√°veis como foi visto anteriormente, tem sua origem na causa de algum outro evento que n√£o conhecemos, rotulado por ($?$). S√≥ sabemos que seu comportamento tem uma influ√™ncia direta sobre a magnitude do efeito dos div√≥rcios e da quantidade de lojas da rede de restaurantes tem por regi√£o.   
# 
# 
# Ent√£o, se estamos tentando medir a liga√ß√£o entre a taxa de casamentos ($M$) e a taxa de div√≥rcios ($D$) usando, como sugest√£o, a estrutura o grafo da direita (*da figura acima*), teremos uma grande confus√£o. Pois  mais adiante uma defini√ß√£o melhor sobre essa confus√£o. 

# Vamos rotular essa estrutura com a seguinte nota√ß√£o: 
# 
# $$ M <-> D | A $$
# 
# Significa que podemos conhecer a `associa√ß√£o` que $M$ e $D$ tem entre si, ($M <-> D$), condicionado ($|$, caracter chamado de *pipe*) a $A$.
# 
# Quando conhecemos o valor de $A$, ent√£o eu tenho um valor extra, uma associa√ß√£o externa, entre as duas vari√°veis. E √© isso que a regress√£o m√∫ltipla pode pode nos dizer. 

# $$ D_i \sim Normal(\mu_i, \sigma)$$
# 
# $$ \mu_i = \alpha + \beta_M M_i + \beta_A A_i $$
# 
# Onde:
# 
# - $D_i$: Taxa de div√≥rcio
# 
# As taxas m√©dias dos casamentos:
# 
# - $\beta_M$: A '*peso*' da taxa de casamentos
# 
# - $M_i$: A taxa m√©dia de casamentos
# 
# 
# As as medianas das idades nos casamentos:
# 
# 
# - $\beta_A$: A '*peso*' da mediana das idades dos casamentos
# 
# - $A_i$: Mediana das idades dos casamentos

# Podemos ver que esse modelo tem uma certa diferen√ßa dos modelos que vimos anteriormente. Existe um $\beta$ para cada uma das vari√°veis preditoras. 
# 
# Essa regress√£o linear, √© um tipo especial de *rede bayesiana*, associa a vari√°vel resposta ($D_i$) a uma distribui√ß√£o gaussiana. E sua m√©dia ($\mu_i$) √© dada pela vari√°veis j√° conhecidas e um desvio padr√£o ($\sigma$).
# 
# Ent√£o, nossa m√©dia ser√° descrita como $\mu_i$, onde $i$, significa que condicionaremos os valores de $\mu_i$ para cada $i$. Aqui, no exemplo, cada $i$ indica um novo estado do sul dos EUA. E, assim, cada condicionamento ser√° descrito pelos '*pesos*' multiplicado as suas pr√≥prias vari√°veis preditoras. 

# ## Prioris

# Agora, n√≥s iremos padronizar a vari√°vel *taxa de div√≥rcio* ($D_i$), a vari√°vel *taxa de casamento* ($M_i$) e tamb√©m a vari√°vel *mediana das idades dos casamentos*.
# 
# Assim, como esperamos, podemos escrever a priori para $\alpha$ da seguinte forma:
# 
# $$\alpha \sim Normal(0, 0.2)$$
# 
# Pois esperamos que $\alpha$ esteja pr√≥ximo de zero!
# 
# E as prioris para os $\beta$'s da seguinte forma:
# 
# $$ \beta_M \sim Normal(0, 0.5)$$
# 
# $$ \beta_A \sim Normal(0, 0.5)$$
# 
# $$ \sigma \sim Exponential(1) $$
# 
# Pois todos os valores foram padronizados, e assim esperamos que a estimativa √† priori seja pr√≥xima de zero.

# In[13]:


# =============================
#   Padronizando as vari√°veis 
# =============================
M_stdr = (df.Marriage - df.Marriage.mean())/df.Marriage.std() 
D_stdr = (df.Divorce - df.Divorce.mean())/df.Divorce.std()
A_stdr = (df.MedianAgeMarriage - df.MedianAgeMarriage.mean())/df.MedianAgeMarriage.std()


# In[14]:


# ===========================================================
#  Priori preditiva para Mediana das Idades dos Casamentos
# ===========================================================

range_A = np.linspace(A_stdr.min(), A_stdr.max())  # Range de valores da m√©dia de idade dos casamentos padronizados

qtd_amostras = 50  # Quantidade de amostras da priori preditiva

# Amostrando os valores de ùúáùëñ da priori
ùõº = np.random.normal(0, 0.2, qtd_amostras)
ùõΩ_A = np.random.normal(0, 0.5, qtd_amostras)
ùúé = np.random.exponential(1, qtd_amostras)  # Priori ùúé ~ Exp(1)


# In[15]:


# ========================================
#   Gr√°fico da Priori Preditiva para ùúáùëñ
# ========================================

ùúá = [ ùõº + ùõΩ_A * A_i for A_i in range_A ]  # Calculando ùúáùëñ = ùõº + ùõΩùê¥ * ùê¥ùëñ, para todo ùê¥ùëñ

# Plotando o gr√°fico 
plt.figure(figsize=(17, 9))

# Relembrando o DAG: A <-> D!
# Plotando as retas da posteriori para ùúáùëñ
plt.plot(range_A, ùúá, color='darkblue', linewidth=0.2)

plt.title("Priori Preditiva de $\mu_i$")
plt.xlabel("( $A$ ) M√©dia das Idades dos Casamentos (padronizadas)")
plt.ylabel("( $D$ ) Taxa de Div√≥rcio (padronizadas)")

plt.grid(ls='--', color='white', alpha=0.4)

# Ajustando os limites da visuzaliza√ß√£o do gr√°fico.
plt.ylim((-2, 2))
plt.xlim((-2, 2))

plt.show()


# Temos no gr√°fico acima a representa√ß√£o da nossa distribui√ß√£o √† priori do modelo de regress√£o linear. Sorteamos $50$ linhas dessa distribui√ß√£o. Essa √© a distribui√ß√£o  √† priori preditiva de $\mu_i$.  Ou seja, `isso √© oque nosso modelo pensa sobre o problema, antes de darmos os dados √† ele`.  
# 
# Temos que sempre fazer a simula√ß√£o da priori preditiva. S√≥ assim ser√° poss√≠vel entender uma importante caracter√≠stica presente em todos os modelo. Temos observar se a amplitude da priori faz sentido. Quando n√≥s dermos os dados para o modelo, √© necess√°rio que as estimativas do modelo resida sobre distribui√ß√£o √† priori. Caso contr√°rio, teremos uma priori ruim. Em cen√°rios mais simples, e com muitos dados, esse efeito pode at√© passar despercebido e ajustar bem. Por√©m em cen√°rios mais complexos, uma priori ruim poder√° ser desastrosa!
# 
# ```{note}
# Sempre verifique a priori.
# ```
# 
# Vamos andar pelo gr√°fico para entendermos melhor suas partes. Iniciando pela `M√©dia das Idades dos Casamentos padronizada ($A$ padronizada)` que √© uma gaussiana, centrada em $0$ e com desvio padr√£o$\frac{1}{2}$:
# 
# $$A \sim Normal(0, 0.5)$$
# 
# No gr√°fico n√≥s estamos observando todos os valores dentro do intervalo $[-2, \mbox{ } 2]$. Esse intervalo corresponde ao $2^o$ desvio padr√£o! Isso deve corresponder a grande maioria dos casos que os valores da Mediana das Idades dos Casamentos (*padronizadas*) ir√° ocorrer. 
# 
# Esse pensamento tamb√©m √© v√°lido para a vari√°vel `Taxa de Div√≥rcio ($D$ padronizada)`. Seus valores tamb√©m est√£o delimitados pelo intervalo $[-2, \mbox{ } 2]$, representando o $2^o$ desvio padr√£o.
# 
# Esse modelo nos diz √© qual a taxa de div√≥rcio padronizada,($D$), que √© uma vari√°vel *z-score*, quando j√° conhecemos os valores da M√©dia das Idades dos Casamentos ($A$), que tamb√©m est√° padronizada (outra *z-score*). Assim temos duas vari√°veis padronizadas, *z-scores*, explicando uma √† outra.
# 
# Agora, se as linhas da regress√£o linear n√£o morar no espa√ßo delimitado pela priori, ent√£o a `escolhemos uma priori ruim`! Por que isso √© imposs√≠vel! Se ap√≥s dermos os dados pra o modelo, a inclina√ß√£o da curva for maior que a delimitada pela regi√£o da priori, temos uma priori ruim.
# 
# Ainda podemos discutir se a escolha da priori ir√° governar todo o range da taxa de div√≥ricio. Provavelmente n√£o √© verdade. Em cap√≠tulos posteriores, quando falarmos sobre *overfitting* (ou em portugu√™s, teremos um sobreajuste). Isso acontece pois a priori n√£o √© apertada o suficiente para trabalharmos seguran√ßa. Nesse caso podemos pensar em prioris *flat*, que podem ser justificada cientificamente, tal como uma priori *flat* impl√≠cita nas an√°lises frequentistas. 
# 

# ## Nosso modelo

# Esse √© nosso modelo, descrito na primeira linha. Na segunda linha temos dois termos lineares, lembrando que o termo linear significa aditivo. Esses dois termos ir√° gerar um plano. A seguir, teremos nossas prioris. Um destaque para a priori do $\sigma$, de agora em diante n√£o vamos mais usar $Uniform$ mas sim a distribui√ß√£o $Exponential$, pois ela tem boas propriedades. Falaremos mais sobre isso adiante.
# 
# $$D_i \sim Normal(\mu_i, \sigma) $$
# 
# $$ \mu_i = \alpha + \beta_M M_i + \beta_A A_i $$
# 
# $$ \alpha \sim Normal(0, 0.2) $$
# 
# $$ \beta_M \sim Normal(0, 0.5) $$
# 
# $$ \beta_A \sim Normal(0, 0.5) $$
# 
# $$ \sigma \sim  Exponential(1) $$
# 
# 
# O uso da distribui√ß√£o $Exponential$ como priori para $\sigma$ tem vantagens tais como, sempre tem valores positivos, para valores maiores a sua probabilidade decresce e para defini-l√° precisamos indicar qual ser√° seu valor m√©dio.

# In[16]:


# =====================================
#  Estimando o modelo linear proposto
# =====================================

stan_model_divorce = """
    data {
        int N;
        vector[N] divorce_rate;
        vector[N] marriage_rate;
        vector[N] median_age;
    }
    
    parameters {
        real alpha;
        real beta_M;
        real beta_A;
        real<lower=0> sigma;
    }

    model{
        alpha ~ normal(0, 0.2);
        beta_M ~ normal(0,0.5);
        beta_A ~ normal(0, 0.5);
        sigma ~ exponential(1);
        
        divorce_rate ~ normal(alpha + 
                              beta_M * marriage_rate + 
                              beta_A * median_age, 
                              sigma);
    }
"""

my_data = {
    'N': len(D_stdr.values), 
    'divorce_rate': D_stdr.values,
    'marriage_rate': M_stdr.values,
    'median_age': A_stdr.values
}

posteriori_divorce = stan.build(stan_model_divorce, data=my_data)
fit_divorce = posteriori_divorce.sample(num_chains=4, num_samples=1000)

alpha = fit_divorce['alpha'].flatten()
beta_M = fit_divorce['beta_M'].flatten()
beta_A = fit_divorce['beta_A'].flatten()
sigma = fit_divorce['sigma'].flatten()


# In[17]:


# Iremos usar esses valores mais tarde nesse material, quando formos fazer a verifica√ß√£o.
# Assim,irei renomear as vari√°veis da posteriori.

alpha_check = fit_divorce['alpha'].flatten()
beta_M_check = fit_divorce['beta_M'].flatten()
beta_A_check = fit_divorce['beta_A'].flatten()
sigma_check = fit_divorce['sigma'].flatten()


# In[18]:


def resume_posteriori(var, confidence_HPDI=0.93, rounded=2):
    """
    Return the summary of posteriori data
    """
    posteriori = []

    confi_HPDI = HPDI(var, confidence_HPDI)
    
    posteriori.append(var.mean())
    posteriori.append(var.std())
    posteriori.append(confi_HPDI[0])
    posteriori.append(confi_HPDI[1])

    return np.round(np.array([posteriori]), rounded)[0]


# In[19]:


def describe_posteriori(vars_post, confidence_HPDI=0.93, plot=True):
    
    post = []
    
    for var_ in vars_post:
        post.append(resume_posteriori(eval(var_), confidence_HPDI))
    
    hpdi_min_label = str(100 * round(1 - confidence_HPDI, 3)) + '%'
    hpdi_max_label = str(100 * round(confidence_HPDI, 3)) + '%'
        
    post = pd.DataFrame(post,
                        index=vars_post,
                        columns=['Mean', 'Std', hpdi_min_label, hpdi_max_label])
    
    if plot:
        
        plt.figure(figsize=(17, len(post) + 1))
    
        plt.title('Estimativas das Posterioris')
        
        min_axis_ = post.iloc[:, 2:4].min().min()
        max_axis_ = post.iloc[:, 2:4].max().max()

        for i in range(len(post)):
            plt.plot([min_axis_*1.5, max_axis_*1.5], [i, i], ls='--', color='gray')
            plt.plot([post.iloc[i, 2], post.iloc[i, 3]], [i, i], color='blue')
            plt.plot(post.iloc[i, 0], i, 'ko')
            plt.annotate(post.index[i], (min_axis_*1.5, i+0.2), color='blue')
            

        if min_axis_ < 0 and max_axis_ > 0:
            plt.axvline(0, ls='--', color='red', alpha=0.6)

        plt.ylim((-1, len(post)+1))
        plt.grid(ls='--', color='white', alpha=0.4)
        
        ax = plt.gca()
        ax.axes.yaxis.set_visible(False)
        
        plt.show()
    
    return post


# In[20]:


# =====================================
#  Descrevendo os dados da posteriori
# =====================================

vars_post = ['alpha', 'beta_M', 'beta_A', 'sigma']
describe_posteriori(vars_post, 0.945, plot=False)


# As informa√ß√µes da tabela acima cont√©m um resumo das posterioris estimadas. Como esperado $\alpha$ tem a m√©dia $0$. Pois tinha que ser assim conforme a constru√ß√£o do nosso modelo.
# 
# J√° a estimativa para do $\beta_M$, `taxa de casamento`, √© levemente negativa e o desvio padr√£o est√° entre $2$ a $3$ vezes a sua m√©dia. Podemos olhar para o intervalo de HPDI de $89\%$ no qual essa valores est√£o entre $-0.36$ e $0.26$. 
# 
# Talvez exista algum efeito, ou talvez n√£o tenha nenhum efeito dependendo da dire√ß√£o. N√≥s apenas n√£o sabemos muito bem o que pensar sobre essa vari√°vel, pois ela n√£o apresenta uma rela√ß√£o consistente, n√£o existe uma associa√ß√£o consistente na regress√£o multipla entre a *taxa de casamentos* e a *taxa de div√≥rcios*.
# 
# Agora, a `mediana das idades do casamentos` tem uma estimativa m√©dia de $-0.6$ com o desvio padr√£o tamb√©m de $0.16$, temos que a massa da posteriori est√° inteiramente abaixo de zero! Assim, realmente temos uma associ√ß√£o negativa entre a *media das idades dos casasmentos* e a *taxa de div√≥rcios*.
# 
# 
# Mas agora n√≥s j√° sabemos que n√£o temos um `impacto diretamente causal` entre *taxa de casamentos* e a *taxa de div√≥rcios*. Isso estava mascarado pois *idade mediana* √© uma causa comum entre as duas outras vari√°veis. 
# 

# In[21]:


#  Legendas
# ===========

# beta_2 == D ~ A   
# beta_A ==> D ~ A + M 

# beta_M ==> D ~ A + M  
# beta_1 ==> D ~ M  

vars_all = ['beta_1', 'beta_M', 'beta_2', 'beta_A'] 
describe_posteriori(vars_all, 0.945, plot=True)


# Quando observando as tabelas acima e comparando com o gr√°fico, temos que a taxa de casamentos ($\beta_M$), resultante da regress√£o multipla, tem a sua distribui√ß√£o √† posteriori, com $89\%$ de HPDI, contendo o `zero`. Portanto prov√°velmente a vari√°vel taxa de casamento ($\beta_M$) `n√£o tem impacto casual direto` na taxa de div√≥rcio. 
# 
# Ele foi mascarado por que a *mediana das idades dos casamentos* ($A$) √© uma vari√°vel que influ√™ncia tanto a *taxa de div√≥rcio* quanto a *taxa de casamentos*.
# 
# No gr√°fico acima fica claro que o modelo *beta_1*, no modelo $D ~ M$, nos diz que *taxa de casamento* tem uma associa√ß√£o positiva com a *taxa de div√≥rcio*, o que era de se esperar pelos nossos entendimentos sobre como poderia funcionar esse evento. 
# 
# Entretanto, como a regress√£o m√∫ltipla podemos ver que a posteriori atinge o zero, e assim n√£o podemos perceber que *provavelmente* n√£o existe uma associa√ß√£o entre a *taxa de casamentos* e a *taxa de div√≥rcios*.
# 
# Isso √© uma boa coisa que a regress√£o m√∫ltipla pode fazer por n√≥s! Eu acredito que `essa √© realmente a rela√ß√£o causal` que temos aqui. 
# 
# Assim, nosso *DAG* provavelmente seria dessa forma:

# In[22]:


# =======================================
#   Construindo o desenho de um DAG 
# =======================================

# Esses c√≥digos devem estar invis√≠veis no jupyter-book 

plt.figure(figsize=(17, 7))

plt.xlim(1, 2)
plt.ylim(0, 1)

size=40

#-------------------------
# DAG - Direita
#-------------------------

plt.annotate('A', (1.2, 0.8), fontsize=size)
plt.annotate('M', (1.8, 0.8), fontsize=size)
plt.annotate('D', (1.48, 0.17), fontsize=size)

plt.title('Grafo dos Div√≥rcios')

# Edge: M <---> D
#lt.annotate("", 
#             xytext=(1.79, 0.79), xy=(1.53, 0.265) ,
#             arrowprops=dict(arrowstyle="-|>,  head_width=1, head_length=1.2"))

# Edge: A <---> M
plt.annotate("", 
             xytext=(1.27, 0.82), xy=(1.77, 0.82),
             arrowprops=dict(arrowstyle="-|>,  head_width=1, head_length=1.2"))

# Edge: A <---> D
plt.annotate("", 
             xytext=(1.24, 0.77), xy=(1.48, 0.26),
             arrowprops=dict(arrowstyle="-|>,  head_width=1, head_length=1.2"))

# Remover os valores dos eixos
plt.yticks([])
plt.xticks([])

plt.show()


# Assim, como n√£o temos uma rela√ß√£o forte da *taxa de casamentos* ($M$) com a vari√°vel *taxa de div√≥rcios* ($D$), a representa√ß√£o de nosso *DAG*, ou seja, a representa√ß√£o da causalidade, poder√° ser mapeada de acordo com o gr√°fico acima. 
# 
# Mas vamos explorar um pouco melhor o significado dessa abordagem.

# ### Regress√£o M√∫ltipla

# - Uma vez que n√≥s conhecemos $A$, *mediana das idades dos casamentos*, existe apenas um pequeno valor adicional de conhecimento sobre $D$ que est√° contido em $M$, a *taxa de casamento*.
# 
# Ent√£o podemos interpretar isso da seguinte forma, ao conhecermos os valores de $A$ implica que o conhecimento de $M$ n√£o vai nos ajudar muito mais a entender $D$. Isso coincide com *DAG* acima, onde n√£o h√° rela√ß√£o causal da *taxa de casamentos* para a *taxa de div√≥rcios*.
# 
# 
# 
# - Uma vez que n√≥s conhecemos a $M$, taxa de casamentos, h√° muito valor em conhecer tamb√©m $A$, mediana das idades dos casamentos. 
# 
# Isso funciona na outra dire√ß√£o, pois a *mediana das idades dos casamentos* ($A$) √© uma `causa` comum da *taxa de casamentos* ($M$).
# 
# 
# - Se n√≥s n√£o conhecermos a *mediana das idades dos casamentos* ($A$) em algum estado, ainda √© √∫til conhecer a *taxa de casamentos* ($M$).
# 
# Saber sobre a *taxa de casamentos* ($M$) √© √∫til e importante pois isso pode nos d√° informa√ß√µes adicionais. Essa informa√ß√£o vem de outra rela√ß√£o causal e n√£o nos d√° uma informa√ß√£o causal direta entre as vari√°veis.

# ```{note}
# Esse √© nosso neg√≥cio aqui. A infer√™ncia! Descobrir a diferen√ßa entre essas coisas.
# ```

# Se n√≥s apenas quisermos fazer uma previs√£o e n√£o nos importarmos com a infer√™ncia causal, a *taxa de casamento* √© √∫til e ajudar√° a prever as coisas. Mas, por outro lado, n√£o nos ajuda a fazer `intereven√ß√µes no mundo real`, porque se quisermos mudar a *taxa de div√≥rcios* ($D$) nos estado manipulando a *taxa de casamento* ($M$) n√£o teria nenhum efeito.
# 
# Simplesmente por que n√£o √© assim que as coisas funcionam. `A maquinaria de causalidade natural dos div√≥rcios n√£o apresenta essa liga√ß√£o` entre da *taxa de casamentos* para a *taxa de div√≥rcios*.
# 
# Assim, para efeito de pol√≠ticas p√∫blicas, √© necess√°rio focar nas altera√ß√µes na *mediana da idade dos casamentos* ($M$) para verificar os efeitos na *taxa de div√≥rcios* ($D$).
# 
# Assim precisamos ser claros se queremos **apenas** prever as coisas. Se quisermos tamb√©m inteferir as rela√ß√µes causais, devemos fazer a previs√£o das rela√ß√µes causais para que possamos fazer as interven√ß√µes. `Uma interven√ß√£o requer um verdadeiro entendimento da causalidade do sistema`. 
# 
# ```{warning}
# Uma previs√£o n√£o tem o poder de fazer tais interven√ß√µes causais.
# ```
# 
# Isso √© o grande terror da ci√™ncia, podemos fazer previs√µes realmente boas sem entendermos nada! Lembra dos modelos *geoc√™ntricos*. `Modelo estat√≠sticos corretos n√£o s√£o suficientes para descobrir rela√ß√µes causais`, ent√£o precisamos de algo extra!

# ## Predi√ß√£o da Posteriori
# 
# Como visualizamos os modelos como este acima? Existem muitas diferentes formas de mostrar isso. Vamos ver algumas  formas e exemplos rapidamente. Por√©m a maneira mais √∫til de visualizar um particular modelo depende do modelo em particular e do objeto de estudo que estamos tentando observar. N√£o existe uma forma gen√©rica para visualizar qualquer tipo de gr√°fico.
# 
# Vamos apresentar alguns exemplos:
# 
# 1. **Plot dos res√≠duos dos preditores**: Muito √∫til para entender como a regress√£o funciona, assim iremos fazer isso apenas uma vez nesse curso. Esse tipo de visualiza√ß√£o √© muito bom para entender o funcionamento da regress√£o mas n√£o √© t√£o bom para comunicar os resultados.
# 
# 
# 
# 2. **Plot contrafactuais**: Chama-se contrafactuais pois n√≥s imaginamos como manipular qualquer uma das vari√°veis sem alterar quaisquer outras vari√°rveis. Com isso faremos as previs√µes para saber como o modelo se comporta com essas manipula√ß√µes.
# 
# 
# 
# 3. **Plot da Predi√ß√£o da Posteriori**: √â basicamente o mesmo que j√° fizemos antes, por√©m veremos alguns pontos diferentes mais adiante.

# ### Plot dos Res√≠duos dos Preditores
# 
# - **Objetivo:** Mostrar a associa√ß√£o de cada um dos preditores com o resultado, "controlado" por outras preditores.
# 
# A associa√ß√£o de uma vari√°vel com o resultado parece ter o controle em outras vari√°veis preditoras, ent√£o dentro da maquinaria do modelo. Queremos com isso entender como o modelo *enxerga* essas coisas internamente. Isso √© o que queremos fazer, calcular esses estados intermedi√°rios mesmo que eles n√£o sejam vis√≠veis no modelo, para termos uma boa intui√ß√£o do que est√° acontecendo internamente no modelo.
# 
# 
# - Nos d√° uma intui√ß√£o muito boa.
# 
# √ötil para termos uma intui√ß√£o do modelo e como est√° o seu funcionamento interno.
# 
# 
# - Nunca analise os res√≠duos.
# 
# N√£o existem motivos l√≥gicos para fazer uma regress√£o da vari√°vel resposta, ($D \sim residuos$, por exemplo), isso nos d√° uma resposta errada. Pois nos dar√° estimativas erradas e tend√™nciosas.
# 
# 
# **Receita**:
# 
# 1. **Fa√ßa uma regress√£o de uma vari√°vel com as outras vari√°veis.**
# 
#     - No nosso exemplo, teremos a *idade mediana dos casamentos*, ($M$) explicando a *taxa de casamento*, ($A$), ambas padronizadas. 
# 
# 
# 
# 2. **Calcule os res√≠duos dos preditores.**
# 
#     - Assim podemos encontrar a *vari√¢ncia extra* que sobrou depois dessa associa√ß√£o, esses s√£o os res√≠duos.
# 
# 
# 
# 
# 3. **Fa√ßa uma regress√£o da vari√°veis resposta com os res√≠duos encontrados no passo anterior.**

# In[23]:


# =============================================
#   Lendo os dados da Waffle House (novamente)
# =============================================

df = pd.read_csv('./data/WaffleDivorce.csv', sep=';')
df.head()


# In[24]:


# ==========================================
#   Relembrando: Padronizando as vari√°veis 
# ==========================================
M_stdr = (df.Marriage - df.Marriage.mean())/df.Marriage.std() 
D_stdr = (df.Divorce - df.Divorce.mean())/df.Divorce.std()
A_stdr = (df.MedianAgeMarriage - df.MedianAgeMarriage.mean())/df.MedianAgeMarriage.std()


# Para evitar a reescrever muito c√≥digo, irei ao longo do texto criando alguns fun√ß√µes para facilitar a leitura.
# 
# Segue abaixo a primeira fun√ß√£o, ela ir√° plotar uma priori para um modelo linear normalmente distribu√≠do.

# In[25]:


# ===================
#  Plotando a priori
# ===================

def plot_priori_lm_normal(N, alpha, beta):
    """
    Plot √† priori to linear model using normal distribution.
    
    Modelo:
    =======
    y ~ normal(mu_i, sigma)
    mi = alpha + beta * x
    
    Prioris:
    ========
    alpha ~ normal(alpha[alpha_mean], alpha[alpha_std], N)
    beta ~ normal(beta[beta_mean], beta[beta_std], N)
    sigma ~ exponential(1)
    
    Params:
    =======
    N: int
    alpha: [alpha_mean, alpha_std]
    beta: [beta_mean, beta_std]
    """
    # Prioris
    alpha = np.random.normal(alpha[0], alpha[1], N)
    beta = np.random.normal(beta[0], beta[1], N)   

    # Os dados originais est√£o padronizados
    x = np.linspace(-3, 3, 100)

    # Modelo linear para as prioris
    y = [alpha + beta * x_i for x_i in x]

    # Plotando a priori
    plt.figure(figsize=(17,9))

    plt.plot(x, y, color='darkblue', linewidth=0.1)

    plt.title('Plot das Prioris normalizadas')
    plt.xlabel('(x) - Eixo X')
    plt.ylabel('(y) - Eixo y')

    plt.xlim((-3, 3))
    plt.ylim((-3, 3))

    plt.grid(ls='--', color='white', alpha=0.4)

    plt.show()


# Abaixo, a fun√ß√£o ir√° encapsular todo o c√≥digo de uma regress√£o linear bayesiana univariada.

# In[26]:


# ================================
#  Regress√£o Linear - Univariada 
# ================================

def plot_lm_posteriori(var_y, var_x, alpha, beta, N=200, plot_residuos=True, 
                       title_xaxis=False, title_yaxis=False):
    # Modelo linear para as prioris
    # Os dados originais est√£o padronizados

    # Os dados originais est√£o padronizados
    x = np.linspace(-3, 3, 100)
    
    # C√°culo dos y e y_mean dado x
    y = [alpha + beta * x_i for x_i in x]
    y_mean = alpha.mean() + beta.mean() * x

    # Plotando a priori
    plt.figure(figsize=(17,9))

    plt.plot(x, y, color='darkblue', linewidth=0.1, alpha=0.5)
    plt.plot(x, y_mean, color='black', linewidth=2)
    plt.plot(var_x, var_y, 'o', color='red', markersize=5)

    if plot_residuos:
        plt.plot([var_x, var_x], 
                 [var_y, alpha.mean() + beta.mean() * var_x],
                 '-', markersize=3, color='darkred', alpha=0.3)

    plt.title('Posteriori Plot dos Res√≠duos dos Preditores')
    plt.xlabel(title_xaxis if title_xaxis else '(x) - Eixo x')
    plt.ylabel(title_yaxis if title_yaxis else '(y) - Eixo y')

    plt.xlim((-3, 3))
    plt.ylim((-3, 3))

    plt.grid(ls='--', color='white', alpha=0.4)

    plt.show()

    

def lm(var_y, var_x, alpha, beta, exp=1, num_chains=4, filter_n=500, num_samples=5000, plot=True, 
       plot_residuos=True, title_xaxis=False, title_yaxis=False):
    
    if not (len(var_x) == len(var_y)):  # TODO: Use except to raise an error.
        print('Erro: As dimens√µes das vari√°veis n√£o s√£o iguais.')
        return False
    
    stan_model = """ 
        data {
            int<lower=0> N;
            vector[N] variavel_resposta;
            vector[N] variavel_explicativa;
            real alpha_mean;
            real alpha_std;
            real beta_mean;
            real beta_std;
            real exp;
        }

        parameters {
            real alpha;
            real beta;
            real<lower=0> sigma;
        }

        model {
            alpha ~ normal(alpha_mean, alpha_std);
            beta ~ normal(beta_mean, beta_std);
            sigma ~ exponential({exp});

            variavel_resposta ~ normal(alpha +  beta  * variavel_explicativa, sigma);
        }
    """

    data = {
        'N': len(var_y),
        'variavel_resposta': var_y,
        'variavel_explicativa': var_x,
        'alpha_mean': alpha[0],
        'alpha_std': alpha[1],
        'beta_mean': beta[0],
        'beta_std': beta[1],
        'exp': exp,
    }
    
    posteriori = stan.build(stan_model, data=data)
    fit = posteriori.sample(num_chains=num_chains, num_samples=num_samples)

    alpha_ = fit['alpha'].flatten()
    beta_ = fit['beta'].flatten()
    
    # Filtra a quantidade de dados para o plot
    if filter_n > 0:
        alpha_ = alpha_[len(alpha_) - filter_n: ]
        beta_ = beta_[len(beta_) - filter_n: ]
    
    if plot:
        plot_lm_posteriori(var_y, var_x, alpha_, beta_, plot_residuos=plot_residuos, 
                           title_xaxis=title_xaxis, title_yaxis=title_yaxis)
    
    residuos = var_y - (alpha_.mean() + beta_.mean() * var_x)
    
    return alpha_, beta_, residuos


# In[27]:


# ====================================
#  Plotando a priori para verificar 
# ====================================
# Prioris
alpha_divorce = [0, 0.3]
beta_divorce = [0, 0.4]
N = 200

# Plot da Priori - Modelo linear
plot_priori_lm_normal(N, alpha_divorce, beta_divorce)


# In[28]:


# ==================
#  Posteriori M ~ A
# ==================

posteriori_MA = lm(var_y=M_stdr.values, var_x=A_stdr.values, alpha=alpha_divorce, beta=beta_divorce,
                   title_yaxis= "Taxa de casamentos (M)",
                   title_xaxis= "Mediana das Idades dos Casamentos (A)")


# In[29]:


# =============================
#  Posteriori D ~ residuos_MA
# =============================
residuos_MA = posteriori_MA[2]

posteriori_MA = lm(D_stdr.values, residuos_MA, alpha_divorce, beta_divorce, plot_residuos=False,
                   title_xaxis="Taxa de Div√≥rcio (D)", 
                   title_yaxis="Res√≠duos da Taxa de Casamentos (A-residual)")


# In[30]:


# ===================
#  Posteriori A ~ M
# ===================

posteriori_AM = lm(var_y=A_stdr.values, var_x=M_stdr.values, alpha=alpha_divorce, beta=beta_divorce, 
                   title_yaxis= "Mediana das Idades dos Casamentos (A)",
                   title_xaxis= "Taxa de casamentos (M)")


# In[31]:


# =============================
#  Posteriori D ~ residuos_AM
# =============================
residuos_AM = posteriori_AM[2]  # Res√≠duos

# posteriori_AM = lm(D_stdr.values, residuos_AM, alpha_divorce, beta_divorce, plot_residuos=False)
posteriori_AM = lm(D_stdr.values, residuos_AM, alpha_divorce, beta_divorce, plot_residuos=False,
                   title_xaxis="Taxa de Div√≥rcio (D)", 
                   title_yaxis="Res√≠duos da Idade da Mediana dos Casamentos (M-residual)")


# Podemos observar que nos gr√°ficos acima (o ($2^o$) gr√°fico `Posteriori D ~ residuos_MA` e o ($4^o$) gr√°fico `Posteriori D ~ residuos_AM`) que a regress√£o dos seus respectivos res√≠duos explicando a taxa de div√≥rcio.
# 
# Existe uma forte correla√ß√£o negativa no gr√°fico `Posteriori D ~ residuos_AM`, o que j√° ocorre no gr√°fico `Posteriori D ~ residuos_MA`. Isso ocorre por que assim existe um valor consider√°vel
# da `taxa de casamentos (M)` que explica  
# 
# 
# ------
# 
# (**TODO:** *Comparar com a [aula](https://youtu.be/e0tO64mtYMU?t=2281), 38:01* - Parece que est√£o invertidos.)
# 
# 

# ## 'Controles' estat√≠sticos
# 

# Quando pensamos em controles estat√≠ticos, j√° nos vem a mente os *Designs de Experimentos*, no qual configuramos quais as vari√°veis s√£o poss√≠velmente *causas* nos estudos observados. N√£o fazemos isso, n√£o h√° nada √©tico que indique que *taxa de casamento* ($M$) √© causa de algo. Isso n√£o √© √©tico com as outras pessoas nem conosco fazer essas coisas. 
# 
# Ent√£o, n√≥s est√°vemos presos a estudos observacionais para uma quantidade muito grande de problemas importantes e o que n√≥s queremos fazer √© `inferencias causais` no entanto, e as regress√µes m√∫ltiplas nos oferecem uma forma para fazer isso, apenas quando combinamos com uma ideia clara sobre quais s√£o as rela√ß√µes causais entre os modelos.
# 
# `Controle estat√≠stico` significa condicionar apenas as informa√ß√µes em uma vari√°vel e verificar se existe alguma informa√ß√£o valiosa!
# 
# Mas, para `interpretar o efeito` o efeito que acontece apartir desses controles √© necess√°rio um framework, ou seja, uma estrutura adequada tal como os *DAGs* ou qualquer outra coisa. Iremos ver exemplos que ao controlar algo estamos na verdade criando um `confound` (*uma confus√£o*). Podemos criar confound tanto quanto remov√™-los, nesse caso iremos remover achando que iremos ter uma resposta certa, mais tarde. 

# - **Regress√£o linear m√∫ltipla responde a quest√£o**: O qu√£o cada preditor est√° associado com o resultado, uma vez que conhecemos TODOS os outros preditores?
# 
# 
# - Usamos modelo para construir a sa√≠da esperada - *n√£o m√°gica!*
# 
# 
# - N√£o ser arrogante: A *taxa de casamento* ainda pode estar associada com a *taxa de div√≥rcio* para algum  subconjunto de estados.
#     
#     
# - N√£o podemos fazer uma forte infer√™ncia causal apenas com as m√©dias, √© preciso os dados indiv√≠duais.

# ## Gr√°ficos contrafactuais

# Os gr√°fico contrafactuais s√£o mostrados quando mantemos todas as vari√°veis fixas e manipulamos apeans uma vari√°vel de interesse. Assim, veremos a linha de regress√£o mudando seu comportamento.
# 
# 
# O objetivo √© explorar as implica√ß√µes dos resultados do modelo.
# 
#     - Fixar os outros preditores
#     
#     - Calcular a predi√ß√£o atrav√©s do valor dos preditores
# 
# 
# 
# Nos gr√°ficos vemos como o modelo v√™ as coisa internamente, ou seja, como ele v√™ as rela√ß√µes preditivas, assumindo que podemos brincar de Deus e definir os valores preditos para qualquer rela√ß√£o. Claro, qualquer rela√ß√£o que n√£o esteja no mundo real.
# 
# 
# Manipular uma dessas vari√°veis tamb√©m ir√° manipular os valores das outras vari√°veis. Assim, se manipularmos a `idade mediada dos casamentos` tamb√©m estaremos manipulando a `taxa de casamento`. Por√©m o contr√°rio n√£o √© verdadeiro.
# 
#     
# Com esses gr√°ficos podemos definir qualquer valor que goste e ver como o modelo ir√° reagir a essa altera√ß√£o. Isso √© muito √∫til para saber o acontece no modelo, mas isso ainda `n√£o √© inferencia causal`.

# ## Checagem da predi√ß√£o da posteriori

# - **Objetivo**: Calcular as predi√ß√µes implic√≠ta para os casos observados.
# 
#     - Checar o ajuste do modelo - golems podem cometer erros.
#     
#     - Encontrar falhas na modelagem, estimular novas ideias.
#   
#   
# Para saber a aproxima√ß√£o da posteriori funcionou corretamente. As vezes os ajustes da posteriori nos diram que o ajuste est√° muito ruim entre as `previs√µes posteriores do modelo` e os `dados brutos`. Isso que iremos fazer, comparar esses dois conjuntos de dados, se forem semelhantes, n√≥s podemos ter errado algo, nosso computador pode ter errado algo ou ambos. 
# 
# Precisamos rever e ajustar, as estimativas dos modelos podem estar erradas pois as vezes a natureza do modelo √© complicada deixando um ambiente hostil para vida humana e temos que lutar para existir, lutando contra a entropia.
# 
# 
# ---
# 
# - Sempre sobre a m√©dia da distribui√ß√£o posteriori.
# 
#     - Usar apenas a m√©dia da posteriori nos leva a excesso de confian√ßa.
#     
#     - Abra√ßar a incerteza.
#     
#     
# A outra coisa que podemos fazer √© olhar para os casos que n√£o se encaixam muito bem e tentar descobrir como fazer infer√™ncias causais mais robustas.  

# In[32]:


def plot_comparative_obseved_posteriori():
    list_compared = []
    
    # Para ficar parecido com os resutados da aula, usar o credible_mass menor que 0.68 (muito demorado!)
    
    for i in range(len(M_stdr.values)):
        predict_divorce = np.random.normal(alpha_check + beta_M_check * M_stdr.values[i] + beta_A_check * A_stdr.values[i], sigma_check[i])
        hpdi = HPDI(predict_divorce, credible_mass=0.89)
        mean_posteriori = np.mean(predict_divorce)
        list_compared.append([D_stdr.values[i], mean_posteriori, hpdi[0], hpdi[1]])
    
    list_compared = pd.DataFrame(list_compared, columns=['Observed', 'Predicted', 'HPDI_min', 'HPDI_max'])
        
    plt.figure(figsize=(17, 9))
    
    # Plot mean
    plt.plot(list_compared.Observed, list_compared.Predicted, 'o', ms=7)
    
    for i in range(len(list_compared)):
        plt.plot([list_compared.Observed[i], list_compared.Observed[i]],  
                 [list_compared.HPDI_min[i], list_compared.HPDI_max[i]],
                 lw=0.4, color='blue')
        
    plt.plot(np.linspace(-2, 2), np.linspace(-2, 2), '--', color='darkblue', linewidth=3, alpha=0.5)

    plt.title('Valores da predi√ß√£o comparada com o observado')
    plt.xlabel('Div√≥rcio Observado')
    plt.ylabel('Div√≥rcio Predito')

    plt.grid(ls='--', color='white', linewidth=0.4)
    plt.show()

    
plot_comparative_obseved_posteriori()


# O gr√°fico acima ir√° comparar os valores preditos contidos na posteriori com os valores observados. Para cada um dos estados, calculamos o intervalo de $89\%$ de credibilidade (HPDI) e sua m√©dia. 
# 
# 
# A linha tracejada √© a identidade. Para estados que tem a m√©dia em cima da linha, temos um ajuste perfeito. J√° para estados longe dessa linha, como o estado ID ([Idaho](https://pt.wikipedia.org/wiki/Idaho)) o valor predito falha! 
# 
# 
# Uma das poss√≠veis explica√ß√µes do porque estamos prevendo altos valores na taxa de div√≥ricos em um estado que temos como evid√™ncia uma taxa bem mais baixa √©, a grande presen√ßa de frequentadores da `A Igreja de Jesus Cristo dos Santos dos √öltimos Dias`, conhecidos como os [M√≥rmons](https://pt.wikipedia.org/wiki/Idaho#/media/Ficheiro:Idaho_Falls_Temple.jpg). Casamentos de pessoas dessa religi√£o tendem a serem mais duradouros por seus motivos religiosos.
# 
# Mas para outros estados que est√£o mal ajustados, √© necess√°rio olhar mais de perto e entender melhor o que est√° acontecendo. 

# ## Associa√ß√µes Mascaradas

# Vamos ver outra coisa interessante que a regress√£o pode fazer. Uma delas √© conseguir revelar as `correla√ß√µes esp√∫rias`, como acabamos de ver na sess√£o anterior, com a estratifica√ß√£o dos controles estat√≠sticos. 

# Outra coisa interessante √© a de que quando temos 2 preditores influenciando na vari√°vel resposta, cada em diferentes dire√ß√µes, eles acabam mascarando um ao outro. Precisamos saber qual o real efeito total da causalidade, chamamos isso de `associa√ß√£o mascarada`.
# 
# 
# - As vezes a associa√ß√£o entre o resultado e o preditor pode ser mascarada por uma outra vari√°vel.
# 
# 
# - Necessitamos de ambos preditores para ver a sua influ√™ncia causal.
# 
# 
# 
# - Tende a surgir quando:
#     
# 
#     - Outro preditor associado com a resposta que atua na dire√ß√£o oposta.    
# 
#     - Ambos os preditores tem uma associa√ß√£o entre si.
# 
# E como consequ√™ncia, na natureza eles escondem o efeito um dos outros. E, se n√£o medirmos ambos, podemos ser levados a acreditar que nenhum dele importa tanto quando realmente importam.  
# 
# 
# - Ru√≠do nos preditores tamb√©m poder√° mascarar a associa√ß√£o (*residuals confounding*).
# 
# Outro tipo de associa√ß√£o mascarada √© quando ocorre que nossas medidas cont√©m muitos erros, assim podemos n√£o ver qual o efeito real que est√° acontecendo. 

# ## Milk and Brain
# 
# <img src="./images/milk_brain.png" alt="Milk and Brain">
# 
# ----
# 
# Dados pode ser encontrado [aqui])(https://github.com/rmcelreath/rethinking/blob/master/data/milk.csv)

# Na imagem acima temos tr√™s primatas, um l√™mure e dois macacos.
# Estamos interessado, com esse conjunto de dados, em saber a associa√ß√£o que existe entre a `energia do leite` ($\frac{Kcal}{gramas}$), qu√£o energ√©tico √© o leite que chegam aos seus filhotes e `qu√£o inteligente` ($\%$ *neocortex*) cada primata √©, usando como particular m√©todo de metrifica√ß√£o de intelig√™ncia, qual o percentual do neocortex em rela√ß√£o a todo o c√©rebro).
# 
# 
# Uma das hip√≥tese que temos √© que, para primatas que sejam mais inteligentes, √© necess√°rio maior quantidade de energia no leite materno.

# In[33]:


# =======================
#  Milk and Brain - Data
# =======================
milk_full = pd.read_csv('./data/milk.csv', sep=';')
milk_full


# In[34]:


# ================
#   Descri√ß√µes
# ================

milk_full.describe()


# In[35]:


# =====================
#  Filtrando os dados 
# =====================

milk = milk_full[['kcal.per.g', 'neocortex.perc']].copy()
milk['log(mass)'] = np.log(milk_full[['mass']])
milk.head()


# In[36]:


# =====================
#  Leite dos Primatas
# =====================

pd.plotting.scatter_matrix(milk[['kcal.per.g', 'log(mass)', 'neocortex.perc']], 
                           figsize=(17, 9), marker='o', color='red',
                           hist_kwds={'bins': 20, 'rwidth': 0.9, 'color': 'red', 'alpha': 0.1}, 
                           s=100)
plt.show()


# O que vemos aqui √© uma forte correla√ß√£o positiva entre o  $\%$ *neocortex* e a magnetude da massa. Ou seja, quanto mais massa o primata tiver, maior tamb√©m ser√° o percentual do seu cortex, e n√£o apenas o c√©rebro como um todo.
# 
# A seguir vamos fazer a regress√£o entre essas duas vari√°veis.

# 
# Parei no 52:51.
# 
