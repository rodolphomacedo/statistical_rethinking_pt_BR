#!/usr/bin/env python
# coding: utf-8

# # 5 - Muitas Vari√°veis e os Waffles Esp√∫rios

# Nosso objetivo nessa parte √© come√ßar o processo da constru√ß√£o de modelos de regress√µes m√∫ltiplas e, assim, tamb√©m come√ßaremos a criar as bases para o `framework de infer√™ncias causais`.
# 
# Para iniciarmos a discuss√£o iremos introduzir um exemplo emp√≠rico, ou seja, um exemplo baseado na experi√™ncia e observa√ß√µes, sejam elas baseadas em algum m√©todo (*met√≥dicas*) ou n√£o.

# <img src="./images/waffle_house.jpeg" alt="Waffle House" width=1000 />
# 
# [Fonte](https://br.linkedin.com/company/waffle-house)

# 
# [Aula - Statistical Rethinking Winter 2019 Lecture 05](https://www.youtube.com/watch?v=e0tO64mtYMU)

# A *waffle house* √© uma cadeia de restaurantes com mais de $2000$ locais em $25$ estados nos EUA (mapa amarelo abaixo). A maioria das suas localiza√ß√µes est√° no Sul do pa√≠s e √© um item da cultural e regional norte americana. (*[wikipedia](https://en.wikipedia.org/wiki/Waffle_House)*). Uma das particularidades dessa rede de restaurantes √© que eles trabalham 24 horas. Nunca fecham! Essa √© a proposta de neg√≥cio deles.
# 
# Um outro fato importante e desagrad√°vel que se surge no sul dos EUA s√£o os *Furac√µes*. Esses s√£o causados pelas depress√µes tropicais clim√°ticas do pais. Os restaurantes da rede Waffles s√£o um dos √∫nicos estabelecimentos que continuam abertos nesses per√≠odos turbulentos. *Exceto quando esses furac√µes atingem uma de suas lojas.*
# 
# A rede √© t√£o confi√°vel que governadores dos EUA criaram o `waffle house index`, internamente na FEMA (*ag√™ncia federal de gest√£o de emerg√™ncias*). Usada como uma m√©trica informal com o nome da rede de restaurantes, pretendem  determinar o efeito das tempestades, como uma escala aux√≠liar para o planejamento de recupera√ß√£o de um desastre. (*[waffle house index](https://en.wikipedia.org/wiki/Waffle_House_Index)*)

# <img src="./images/WH_per_people.png" alt="waffle house map" width=900 />
# 
# Imagem - https://www.scottareynhout.com/blog/2017/10/7/waffle-house-map

# Al√©m dos desastres naturais, existem tamb√©m muitas outras coisas acontecendo em grande escala no sul dos EUA, tal como **div√≥rcios**!
# 
# No gr√°fico abaixo temos a indica√ß√£o da quantidade de div√≥rcios nos EUA. Observe o sul do pa√≠s e compare com o mapa acima.

# <img src="./images/WH_per_divorce.png" alt="Waffle House contry-level-marriage-divorce-data-2010" width=900 />
# 
# Imagem - https://www.bgsu.edu/ncfmr/resources/data/original-data/county-level-marriage-divorce-data-2010.html

# Percebeu? 
# 
# Em ambos os mapas existem uma grande concentra√ß√£o, no extremo do sul do mapa, de restaurantes da rede Waffle House e, subindo mais ao norte do pa√≠s, temos quantidades cada vez menores. O mesmo ocorre no mapa das *taxas de div√≥rcios*, quando olhamos para os mesmos locais no mapa.
# 
# Podemos ent√£o fazer uma estimativa: `esses dados est√£o correlacionados entre si`. E podemos nos ser levados a pensar que quanto maior a concentra√ß√£o de restaurantes na regi√£o maior seria seria a taxa de div√≥rcios.
# 
# E por que isso acontece? 
# 
# Pelo seguinte motivo: `Por nada`! 
# 
# Isso mesmo, nada!!!
# 
# N√£o existe nada que tenha uma rela√ß√£o direta na qual a quantidade de restaurantes da rede em alguma determinada regi√£o influencie casais a brigarem e tomarem a decis√£o de se separar! 
# 
# √â estranho. √â c√¥mico. √â intrigante. Isso √© uma `Correla√ß√£o Esp√∫ria`!

# # Correla√ß√µes Esp√∫rias
# 
# Essas s√£o as `correla√ß√µes esp√∫rias`, ou seja, `correla√ß√µes sem certeza`; que n√£o √© verdadeira nem real; √© hipot√©tica!
# 
# Muita coisa est√° relacionada com as outras no mundo real. `Isso √© a Natureza`!
# 
# Por exemplo, se quisermos, por qualquer motivo que seja, arrumar um "argumento" para enfraquecer a imagem da rede Waffle House, "podemos" usar essas correla√ß√µes esp√∫rias com um dos argumentos. Assim, n√≥s iriamos expor na m√≠dia a seguinte manchete: 
# 
# ```{admonition} Breaking News:
# Pesquisadores [da universidade xyz] indicam que: o aumento do n√∫mero de restaurantes da rede Waffle House 
# na regi√£o implica num aumento assustador no n√∫mero de div√≥rcios nessa mesma regi√£o!
# ```
# 
# 
# Isso soa estranho, eu sei! Esse √© apenas um exemplo extremo.
# 
# Mas, l√° no fundo, esse tipo de pensamento n√£o soa t√£o estranho no dia-a-dia...

# Existem diversas correla√ß√µes esp√∫rias no mundo. Muita coisa tem correla√ß√£o com muitas outras coisas.
# 
# 
# ```{admonition} Entretanto:
# Essas correla√ß√µes n√£o implicam causalidade.
# ```

# Mas para entendermos melhor, vamos ver mais alguns exemplos sobre essas correla√ß√µes esp√∫rias:
# 
# 
# - O consumo de queijo tem uma correla√ß√£o de $94.7\%$ com os acidentes fatais com o emaranhado do len√ßol de cama. 
# 
# 
# - O consumo per capita de frango apresenta uma correla√ß√£o de $89\%$ com a importa√ß√£o de petr√≥leo.
# 
# 
# - Os acidentes por afogamentos em piscina tem a correla√ß√£o de $66\%$ com o n√∫mero de filmes lan√ßados pelo Nicolas Cage, por ano. Veja graficamente essa correla√ß√£o abaixo:
# 
# 
# <img src="./images/chart.jpeg" alt="Tyler Vigen spurious correlations" width=1000>
# 
# 
# 
# 
# Percebeu?
# 
# Se o consumo de frango diminu√≠sse, a importa√ß√£o provavelmente n√£o sofreria nenhum impacto por essa causa. E, caso o consumo de queijo diminuir, tamb√©m n√£o haver√° uma diminui√ß√£o nos acidentes fatais das pessoas que est√£o dormindo em suas camas. E, como √© esperado, se o Nicolas Cage se aposentar dos cinemas, os acidentes por afogamento continuar√£o constantes. 
# 
# ```{tip}
# Correla√ß√£o n√£o implica causalidade!
# ```
# 
# 
# 
# Por fim, ter mais lojas da rede Waffle House n√£o `causa` mais div√≥rcios na regi√£o.
# 
# 
# 
# ----
# Mais correla√ß√µes esp√∫rias, tais como essas acima, podem ser encontradas no site do [Tyler Vigen](https://www.tylervigen.com/spurious-correlations).
# 
# 
# -----
# 
# Entendido essa parte, vamos ao objetivo desse cap√≠tulo.
# 

# # Regress√£o M√∫ltiplas
# 
# Vamos ver como construir um modelo de regress√£o linear novamente. Mas dessa vez iremos ver tamb√©m como se faz com  `m√∫ltiplas vari√°veis` e quais s√£o suas implica√ß√µes. 
# 
# ## Pr√≥s e contras das m√∫ltiplas vari√°veis:
# 
# - A parte boa desse tipo de modelo √© que as regress√µes m√∫ltiplas podem n√£o s√≥ revelar correla√ß√µes esp√∫rias como tamb√©m podem revelar associa√ß√µes escondidas que n√≥s n√£o far√≠amos normalmente, ou seja, n√£o ter√≠amos visto essas associa√ß√µes usando o modelo com uma simples vari√°vel preditora.
# 
# 
# - Mas, por outro lado, podemos tamb√©m adicionar vari√°veis explicativas aos modelos que contenham correla√ß√µes esp√∫rias √† regress√£o m√∫ltipla e, podem tamb√©m, esconder as algumas das reais associa√ß√µes que existem.
# 
# 
# Ent√£o, como essas coisas geralmente n√£o s√£o bem explicadas, vamos detalhar todo o processo de constru√ß√£o de uma regress√£o m√∫ltipla, a seguir.
# 
# Quando constru√≠mos um modelo usando uma regress√£o m√∫ltipla, n√≥s devemos ter uma estrutura mais profusa para pensar sobre as nossas decis√µes. Apenas *jogar* todas as vari√°veis explicativas dentro da regress√£o m√∫ltipla, como usualmente √© feito, √© o segredo para o fracasso da an√°lise e n√≥s n√£o queremos fazer isso!
# 
# Para isso precisamos de uma estrutura mais ampla e rica para conseguirmos pensar e tomar melhores decis√µes. Essa estrutura √© o `framework de infer√™ncia casual`.
# 
# Nessa parte iremos aprender do b√°sico sobre infer√™ncia casual:
# 
# - Grafos ac√≠clicos direcionados (DAGs - *Directed acyclic graphs*)
# 
# 
# - Fork, pipes, colliders...
# 
# 
# - Crit√©rio de Backdoor.

# J√° sabemos que o Waffle House n√£o causa os div√≥rcios. Mas o que causa os div√≥rcios?
# 
# J√° vimos no mapa acima que div√≥rcios do sul do pa√≠s tem uma taxa bem mais alta do que no restante mais ao norte. Existem muitos esfor√ßos para tentar identificar as causas e as taxas de div√≥rcios. Sabemos que no sul tem uma predomin√¢ncia religiosa quando comparada ao restante do pa√≠s. Isso deixa os cientistas com certas desconfian√ßas.

# Existem assim muitas coisas que est√£o correlacionadas com a taxa de div√≥rcios. Uma delas √© a `taxa de casamentos`. Tamb√©m podemos usar essas informa√ß√µes para cada um dos outros $50$ estados do pa√≠s. Todos eles t√™m uma `correla√ß√£o positiva` da taxa de casamentos com a taxa de div√≥rcios.
# 
# Um ponto importante nessa correla√ß√£o √© que *s√≥ pode acontecer um div√≥rcio se houver o casamento*! 
# 
# Mas a correla√ß√£o entre essas taxas podem ser tamb√©m `correla√ß√µes esp√∫rias`. Assim como a correla√ß√£o entre homic√≠dos e div√≥rcios s√£o uma correla√ß√£o esp√∫ria.
# 
# Pois uma taxa alta de casamentos pode indicar que a sociedade v√™ o casamento de modo favor√°vel e isso pode significar taxas de div√≥rcios mais baixas. N√£o necessariamente faz sentido, mas pode ser que as taxas de casamentos e de div√≥rcios sejam correla√ß√µes esp√∫rias tamb√©m. 
# 
# ```{note}
# - Correla√ß√£o n√£o implica Causalidade
# 
# 
# - Causalidade n√£o implica Correla√ß√£o
# 
# 
# - Causalidade implica em correla√ß√£o condicional
# ```
# 
# 
# Assim, precisamos mais do que apenas simples modelos! √â necess√°rio um estrutura mais robusta.
# 
# 
# Mas, o que causa os div√≥rcios? 
# 
# Vamos descobrir isso...

# Existe outra vari√°vel que tamb√©m √© correlacionada com a vari√°vel `taxa de div√≥rcio`, √© a vari√°vel `idade mediana das pessoas que se casam, em cada estado`. Mas diferente da `taxa de casamento`, essa vari√°vel apresenta uma *correla√ß√£o negativa*.

# In[1]:


import numpy as np
import pandas as pd
import stan
import nest_asyncio
import matplotlib.pyplot as plt


# In[2]:


# Definindo o plano de fundo cinza claro para todos os gr√°ficos feitos no matplotlib
plt.rcParams['axes.facecolor'] = 'lightgray'


# In[3]:


# Desbloqueio do asyncIO do jupyter
nest_asyncio.apply()


# In[4]:


def HPDI(posterior_samples, credible_mass):
    
    # Calcula o maior intervalo de probabilidades a partir de uma amostra
    
    # Fonte: https://stackoverflow.com/questions/22284502/highest-posterior-density-region-and-central-credible-region
    sorted_points = sorted(posterior_samples)
    ciIdxInc = np.ceil(credible_mass * len(sorted_points)).astype('int')
    nCIs = len(sorted_points) - ciIdxInc
    ciWidth = [0]*nCIs
    
    for i in range(0, nCIs):
        ciWidth[i] = sorted_points[i + ciIdxInc] - sorted_points[i]
        HDImin = sorted_points[ciWidth.index(min(ciWidth))]
        HDImax = sorted_points[ciWidth.index(min(ciWidth))+ciIdxInc]

    return(HDImin, HDImax)


# In[5]:


# ====================================
#   Lendo os dados da Waffle House
# ====================================

df = pd.read_csv('./data/WaffleDivorce.csv', sep=';')
df.head()


# In[6]:


# ============================================
#  Plotando os dados das taxas de div√≥rcios
# ============================================
fig, [ax1, ax2] = plt.subplots(1, 2, figsize=(17, 9))

plt.suptitle('Associa√ß√µes Esp√∫rias?')

ax1.scatter(df.Marriage.values ,df.Divorce.values)
ax1.grid(ls='--', color='white', linewidth=0.4)
ax1.set_xlabel('Taxa de Casamento')
ax1.set_ylabel('Taxa de Div√≥rcio')

ax2.scatter(df.MedianAgeMarriage.values ,df.Divorce.values)
ax2.grid(ls='--', color='white', linewidth=0.4)
ax2.set_xlabel('Mediana da Idade dos Noivos')
ax2.set_ylabel('Taxa de Div√≥rcio')

plt.show()


# Vamos construir modelos lineares simples para os dois gr√°ficos acima.

# In[7]:


# ========================================================
#  Construindo um modelo linear simples:
#
#    taxa de div√≥rcio ~ alpha + beta * taxa_casamento
# ========================================================
divorce_model1 = """
    data {
        int N;
        vector[N] divorce_rate;
        vector[N] marriage_rate;
    }
    
    parameters {
        real alpha;
        real beta;
        real<lower=0, upper=50> sigma;
    }
    
    model {
        divorce_rate ~ normal(alpha + beta * marriage_rate, sigma);
    }
"""

my_data = {
    'N': len(df.Divorce),
    'marriage_rate': df.Marriage.values,
    'divorce_rate': df.Divorce.values,
}

posteriori1 = stan.build(divorce_model1, data=my_data)
fit1 = posteriori1.sample(num_chains=4, num_samples=1000)

alpha_1 = fit1['alpha'].flatten()
beta_1 = fit1['beta'].flatten()
sigma_1 = fit1['sigma'].flatten()


# In[8]:


# ==============================================================
# Construindo um modelo linear simples:
#
#  taxa divorcio ~ alpha + beta * mediana da idade dos noivos
# ==============================================================

stan_model2 = """
    data {
        int N;
        vector[N] divorce_rate;
        vector[N] median_age_marriage;
    }
    
    parameters {
        real alpha;
        real beta;
        real<lower=0, upper=50> sigma;
    }
    
    model {
        divorce_rate ~ normal(alpha + beta * median_age_marriage, sigma);
    }
"""

my_data = {
    'N': len(df.Divorce),
    'divorce_rate': df.Divorce.values,
    'median_age_marriage': df.MedianAgeMarriage.values,
}

posteriori2 = stan.build(stan_model2, data=my_data)
fit2 = posteriori2.sample(num_chains=4, num_samples=1000)

alpha_2 = fit2['alpha'].flatten()
beta_2 = fit2['beta'].flatten()
sigma_2 = fit2['sigma'].flatten()


# In[9]:


# ====================================================================
#  Plotando os dados das taxas de div√≥rcios e suas estimativas m√©dias
# ====================================================================
fig, [ax1, ax2] = plt.subplots(1, 2, figsize=(17, 9))

plt.suptitle('Associa√ß√µes Esp√∫rias?')

# Calculando as regi√µes de HPDI para mu - Taxa de casamentos com 0.92 de credibilidade
range_mu = np.sort(df.Marriage)
posteriori_mu_aux = np.array([[alpha_1 + beta_1 * marriage for marriage in range_mu]])[0]
posteriori_mu_HPDI = np.array([[HPDI(posteriori_marriage, 0.92) for posteriori_marriage in posteriori_mu_aux]])[0]

# Gr√°fico de Div√≥rcio x Casamento
ax1.scatter(df.Marriage.values ,df.Divorce.values)
ax1.fill_between(range_mu, 
                 posteriori_mu_HPDI[:, 0], posteriori_mu_HPDI[:, 1], 
                 color='gray', alpha=0.4)
ax1.plot(df.Marriage.values, 
         alpha_1.mean() + beta_1.mean() * df.Marriage.values,
         color='black')

ax1.grid(ls='--', color='white', linewidth=0.4)
ax1.set_xlabel('Taxa de Casamento')
ax1.set_ylabel('Taxa de Div√≥rcio')
ax1.set_title('Correla√ß√£o Positiva \n Esp√∫ria?')

# ------------

# Calculando as regi√µes de HPDI para mu - Mediana para idade de casamentos com 0.92 de credibilidade
range_mu = np.sort(df.MedianAgeMarriage)
posteriori_mu_aux = np.array([[alpha_2 + beta_2 * mediaAgeMarriage for mediaAgeMarriage in range_mu]])[0]
posteriori_mu_HPDI = np.array([[HPDI(posteriori_mediaAgeMarriage, 0.92) for posteriori_mediaAgeMarriage in posteriori_mu_aux]])[0]

# Gr√°fico de Div√≥rcio x Mediana da Idade dos Casamentos
ax2.scatter(df.MedianAgeMarriage.values ,df.Divorce.values)
ax2.fill_between(range_mu, 
                 posteriori_mu_HPDI[:, 0], posteriori_mu_HPDI[:, 1], 
                 color='gray', alpha=0.4)

ax2.plot(df.MedianAgeMarriage.values, 
         alpha_2.mean() + beta_2.mean() * df.MedianAgeMarriage.values,
         color='black')

ax2.grid(ls='--', color='white', linewidth=0.4)
ax2.set_xlabel('Mediana da Idade dos Noivos')
ax2.set_ylabel('Taxa de Div√≥rcio')
ax2.set_title('Correla√ß√£o Negativa \n Esp√∫ria?')

plt.show()


# Observando os dois gr√°ficos acima, quais dessas correla√ß√µes s√£o `causas plaus√≠veis`? 
# 
# No gr√°fico da esquerda, sabemos que a `taxa de casamento` tem uma associa√ß√£o direta com a `taxa de div√≥rcio`, pois, para se divorciar √© necess√°rio antes ter casado. Mas essa interpreta√ß√£o dos fatos pode estar contaminada pelo nosso senso comum e, assim, uma associa√ß√£o esp√∫ria, nos seria v√°lida. Um dos motivos pra isso acontecer √© que em locais que possuem maiores incentivos religiosos e podem implicar em maiores incentivos para casamentos, podendo, assim, n√£o haver uma causalidade direta com o div√≥rcio.
# 
# J√° no gr√°fico √† direita, a `taxa de div√≥rcio` est√° associada com `mediana das idades dos noivos`, nos apresentando uma associa√ß√£o negativa. Isso pode nos levar a pensar que jovens, ao se casarem, tomam decis√µes muito emotivas e que isso tamb√©m levaria a altas taxas de div√≥rcios. Ou pode ser que essa seja uma associa√ß√£o esp√∫ria e nossas hip√≥teses e nosso senso comum n√£o tenha sentido.

# Assim, mesmo que nossas justificativas sejam bastante `veross√≠meis` a respeito do comportamento da Natureza do evento, n√£o podemos afirmar que essas associa√ß√µes n√£o sejam esp√∫rias.
# 
# Se a identifica√ß√£o de associa√ß√µes esp√∫rias, como no exemplo de filmes do *Nicolas Cage*, `pareciam √≥bvias` de serem identificadas, agora, essas n√£o parecem ser t√£o simples de perceber sua veracidade.
# 
# Portanto, √© necess√°rio construirmos um ferramental que nos permita identificar com mais facilidade e, com um maior grau de certeza, se essas associa√ß√µes s√£o reais ou s√£o esp√∫rias.
# 
# O que queremos fazer agora √© colocar essas duas vari√°veis no mesmo modelo e entender o que isso faz e por que nos revela, quase certamente, que determinada vari√°vel √© uma `impostora`.

# ## M√∫ltiplas causas de div√≥rcios

# <img src="./images/cidade1.jpg" alt="Aprender a taxa casamentos de uma cidade" />
# 
# [Fonte: *Hong Kong - wikipedia*](https://pt.wikipedia.org/wiki/Cidade#/media/Ficheiro:Hong_Kong_Night_Skyline.jpg)
# 
# Como entender o mecanismo das Causas Naturais de div√≥rcio numa cidade grande, por exemplo?

# O que n√≥s queremos saber √© qual o valor de uma `vari√°vel preditora`, uma vez que n√≥s conhecemos os valores das outras vari√°veis preditoras corretamente?
# 
# Todas vari√°veis preditoras s√£o ,em alguma extens√£o, correlacionadas com as outras vari√°veis preditoras e, tamb√©m, com a vari√°vel resposta. Entretanto, elas t√™m `correla√ß√µes parciais` que s√£o reveladoras de informa√ß√µes adicionais dessa estrutura de correla√ß√£o.

# Para entendermos melhor, vamos exemplificar com nosso exemplo sobre div√≥rcios.
# 
# N√≥s gostariamos de aprender mais sobre o valor da `taxa de casamentos`, uma vez que j√° conhecemos a `mediana da idade da taxa dos noivos`.

# ## Entendendo os grafos um pouco mais perto.

# DAG's s√£o o acr√¥nimo para a express√£o ingl√™s `Directed Acyclic Graph`, o que √© portug√™s √© conhecido como `Grafos Ac√≠clicos Direcionados` [(*ver mais em wikipedia*)](https://pt.wikipedia.org/wiki/Grafos_ac%C3%ADclicos_dirigidos).

# In[10]:


# =======================================
#   Construindo o desenho de um DAG 
# =======================================

# -----------------------------------------------------
# Obs: Os pr√≥ximos grafos irei esconder os c√≥digos, 
#      para ficar visualmente melhor. Mas, se quiser,
#      os c√≥digos podem ser encontrados no github.
# -----------------------------------------------------

plt.figure(figsize=(17, 7))

plt.xlim(0, 1)
plt.ylim(0, 1)

size=40

plt.annotate('A', (0.2, 0.8), fontsize=size)
plt.annotate('M', (0.8, 0.8), fontsize=size)
plt.annotate('D', (0.495, 0.2), fontsize=size)

plt.title('Grafo dos Div√≥rcios')

# Edge: M <---> D
plt.annotate("", 
             xytext=(0.79, 0.79), xy=(0.53, 0.265) ,
             arrowprops=dict(arrowstyle="-|>,  head_width=1, head_length=1.2"))

# Edge: A <---> M
plt.annotate("", 
             xytext=(0.24, 0.82), xy=(0.77, 0.82),
             arrowprops=dict(arrowstyle="-|>,  head_width=1, head_length=1.2"))

# Edge: A <---> D
plt.annotate("", 
             xytext=(0.24, 0.77), xy=(0.48, 0.26),
             arrowprops=dict(arrowstyle="-|>,  head_width=1, head_length=1.2"))


# Remover os valores dos eixos
plt.yticks([])
plt.xticks([])

plt.show()


# DAG's s√£o ferramentas heur√≠sticas usadas no framework de infer√™ncias causais. Elas n√£o como ferramentas mec√¢nicas anal√≠ticas que j√° vimos, mas s√£o *incrivelmente* √∫teis para nos auxiliar a pensar melhor e eventualmente tamb√©m nos permite entender muito melhor os modelos mec√¢nicos.
# 
# As setas do *DAG* apontam apenas em uma `dire√ß√£o` e essa dire√ß√µes indica uma `rela√ß√£o de causalidade` entre as vari√°veis analisadas, ou seja, indica que uma vari√°vel tem `influ√™ncia direta` sobre a outra.
# 
# A aus√™ncia de *ciclos* significa que n√£o existem *loops* na causalidade. Mas esses loops podem acontecer sobre o tempo, o que tornaria a an√°lise tamb√©m uma `s√©rie temporal`!
# 
# A representa√ß√£o desses problemas reais em estruturas como *DAG's* podem ser, realmente, muito grandes. Pois, assim, podemos descrever a estrutura no tempo $T$, $\{T_1, T_2, ...\}$, e assim por diante.
# 
# N√≥s, normalmente rotulamos os grafos por dois nomes: `n√≥s` (*nodes*) e `arestas` (*edge*). (Iremos aqui, por conve√ß√£o, usar os nomes em ingl√™s.)
# 
# ```{note}
# Os nodes (n√≥s) s√£o as vari√°veis. J√° os edges (as arestas) representam a rela√ß√£o causal entre os nodes.
# ```

#  As associa√ß√µes que foram levadas em considera√ß√£o, podem  ser inclu√≠das dentro dos modelos de redes bayesianas, que s√£o considerados parte do conjunto de ferramentas de *Machine Learning*. Esses modelos tamb√©m n√£o levam em considera√ß√£o a causalidade, pois n√£o existe um mecanismo interno nesses modelos que leve em considera√ß√£o a dire√ß√£o da causalidade.
# 
# J√° nos *DAG's*, tal mecanismo, existe!
# 
# ```{note}
#  Esse mecanismo nos permite a capacidade de enxergar, atrav√©s das lentes probabil√≠sticas, a influ√™ncia da causa e efeito em diferentes eventos! Isso n√£o √© medir uma associa√ß√£o. √â uma medida para a causalidade! 
# ```
# 
# E isso, faz `toda` a diferen√ßa! 
# 
# 
# *Tamb√©m veremos qual √© a diferen√ßa entre as duas abordagens comnetadas...*

# ## Bons Grafos

# Queremos saber a diferen√ßa entre esses dois grafos abaixo. O da esquerda tem um caminho direto de $A <-> M <-> D$ e, no outro gr√°fico, uma suposi√ß√£o de que temos que a rela√ß√£o causal entre a taxa de casamento ($M$) e a taxa de div√≥rcio ($D$) n√£o existe mais.

# In[11]:


# =======================================
#   Construindo o desenho de um DAG 
# =======================================

# Esses c√≥digos devem estar invis√≠veis no jupyter-book 

plt.figure(figsize=(17, 7))

plt.xlim(0, 2)
plt.ylim(0, 1)

size=40

#-------------------------
# DAG - Esquerda
#-------------------------

plt.annotate('A', (0.2, 0.8), fontsize=size)
plt.annotate('M', (0.8, 0.8), fontsize=size)
plt.annotate('D', (0.48, 0.17), fontsize=size)

plt.title('Grafo dos Div√≥rcios')

# Edge: M <---> D
plt.annotate("", 
             xytext=(0.79, 0.79), xy=(0.53, 0.265) ,
             arrowprops=dict(arrowstyle="-|>,  head_width=1, head_length=1.2"))

# Edge: A <---> M
plt.annotate("", 
             xytext=(0.27, 0.82), xy=(0.77, 0.82),
             arrowprops=dict(arrowstyle="-|>,  head_width=1, head_length=1.2"))

# Edge: A <---> D
plt.annotate("", 
             xytext=(0.24, 0.77), xy=(0.48, 0.26),
             arrowprops=dict(arrowstyle="-|>,  head_width=1, head_length=1.2"))


#-------------------------
# DAG - Direita
#-------------------------

plt.annotate('A', (1.2, 0.8), fontsize=size)
plt.annotate('M', (1.8, 0.8), fontsize=size)
plt.annotate('D', (1.48, 0.17), fontsize=size)

plt.title('Grafo dos Div√≥rcios')

# Edge: M <---> D
#lt.annotate("", 
#             xytext=(1.79, 0.79), xy=(1.53, 0.265) ,
#             arrowprops=dict(arrowstyle="-|>,  head_width=1, head_length=1.2"))

# Edge: A <---> M
plt.annotate("", 
             xytext=(1.27, 0.82), xy=(1.77, 0.82),
             arrowprops=dict(arrowstyle="-|>,  head_width=1, head_length=1.2"))

# Edge: A <---> D
plt.annotate("", 
             xytext=(1.24, 0.77), xy=(1.48, 0.26),
             arrowprops=dict(arrowstyle="-|>,  head_width=1, head_length=1.2"))

# Remover os valores dos eixos
plt.yticks([])
plt.xticks([])

plt.show()


# Regress√µes Lineares, em princ√≠pio, podem nos dizer a diferen√ßa entre essas duas coisas. Mas uma regress√£o bivariada j√° n√£o pode mais. Elas podem nos dar apenas algum conhecimento da associa√ß√£o entre *casamentos* e *div√≥rcios*, mas n√£o podem nos dizer qual a diferen√ßa entre esses dois *DAG's* acima. 
# 
# Mas, porque n√£o? 
# 
# Por que, A tem influ√™ncia tanto na taxa de casamentos ($A <-> M$) quanto em D ($A <-> D$), isso gera uma `correla√ß√£o` entre os eventos *taxa de casamento* ($M$) e *taxa de div√≥rcio* ($D$), mesmo que uma n√£o influencie na outra, como no gr√°fico √† direita. (*Elegante essa explica√ß√£o, hein!*)
# 
# √â um modo bonito de dizer:
# 
# ```{warning}
# Correla√ß√£o n√£o √© Casualidade!
# ```
# 
# Essa mesmo estrutura do *DAG's*, pode ser modelada com as correla√ß√µes esp√∫rias do in√≠cio do cap√≠tulo, *waffle House* vs *taxa de div√≥rcios*

# In[12]:


# =======================================
#   Construindo o desenho de um DAG 
# =======================================

# Esses c√≥digos devem estar invis√≠veis no jupyter-book 

plt.figure(figsize=(17, 7))

plt.xlim(0, 2)
plt.ylim(0, 1)

size=20


plt.title('Grafo dos Div√≥rcios vs Waffle Houses')


#-------------------------
# DAG - Direita
#-------------------------

plt.annotate('?', (0.2, 0.8), fontsize=size+10)
plt.annotate('Quantidade de Lojas da Waffle House por regi√£o', (0.8, 0.8), fontsize=size)
plt.annotate('Taxa de Div√≥rcio por regi√£o', (0.48, 0.17), fontsize=size)


# Edge: ? <---> Haffle House
plt.annotate("", 
             xytext=(0.27, 0.82), xy=(0.77, 0.82),
             arrowprops=dict(arrowstyle="-|>,  head_width=1, head_length=1.2"))

# Edge: ? <---> Div√≥rcio
plt.annotate("", 
             xytext=(0.24, 0.77), xy=(0.48, 0.26),
             arrowprops=dict(arrowstyle="-|>,  head_width=1, head_length=1.2"))

# Remover os valores dos eixos
plt.yticks([])
plt.xticks([])

plt.show()


# A correla√ß√£o que existe entre essas duas vari√°veis como foi visto anteriormente, tem sua origem na causa de algum outro evento que n√£o conhecemos, rotulado por ($?$). S√≥ sabemos que seu comportamento tem uma influ√™ncia direta sobre a magnitude do efeito dos div√≥rcios e da quantidade de lojas da rede de restaurantes tem por regi√£o.   
# 
# 
# Ent√£o, se estamos tentando medir a liga√ß√£o entre a taxa de casamentos ($M$) e a taxa de div√≥rcios ($D$) usando, como sugest√£o, a estrutura o grafo da direita (*da figura acima*), teremos uma grande confus√£o. Pois  mais adiante uma defini√ß√£o melhor sobre essa confus√£o. 

# Vamos rotular essa estrutura com a seguinte nota√ß√£o: 
# 
# $$ M <-> D | A $$
# 
# Significa que podemos conhecer a `associa√ß√£o` que $M$ e $D$ tem entre si, ($M <-> D$), condicionado ($|$, caracter chamado de *pipe*) a $A$.
# 
# Quando conhecemos o valor de $A$, ent√£o eu tenho um valor extra, uma associa√ß√£o externa, entre as duas vari√°veis. E √© isso que a regress√£o m√∫ltipla pode pode nos dizer. 

# $$ D_i \sim Normal(\mu_i, \sigma)$$
# 
# $$ \mu_i = \alpha + \beta_M M_i + \beta_A A_i $$
# 
# Onde:
# 
# - $D_i$: Taxa de div√≥rcio
# 
# As taxas m√©dias dos casamentos:
# 
# - $\beta_M$: A '*peso*' da taxa de casamentos
# 
# - $M_i$: A taxa m√©dia de casamentos
# 
# 
# As as medianas das idades nos casamentos:
# 
# 
# - $\beta_A$: A '*peso*' da mediana das idades dos casamentos
# 
# - $A_i$: Mediana das idades dos casamentos

# Podemos ver que esse modelo tem uma certa diferen√ßa dos modelos que vimos anteriormente. Existe um $\beta$ para cada uma das vari√°veis preditoras. 
# 
# Essa regress√£o linear, √© um tipo especial de *rede bayesiana*, associa a vari√°vel resposta ($D_i$) a uma distribui√ß√£o gaussiana. E sua m√©dia ($\mu_i$) √© dada pela vari√°veis j√° conhecidas e um desvio padr√£o ($\sigma$).
# 
# Ent√£o, nossa m√©dia ser√° descrita como $\mu_i$, onde $i$, significa que condicionaremos os valores de $\mu_i$ para cada $i$. Aqui, no exemplo, cada $i$ indica um novo estado do sul dos EUA. E, assim, cada condicionamento ser√° descrito pelos '*pesos*' multiplicado as suas pr√≥prias vari√°veis preditoras. 

# ## Prioris

# Agora, n√≥s iremos padronizar a vari√°vel *taxa de div√≥rcio* ($D_i$), a vari√°vel *taxa de casamento* ($M_i$) e tamb√©m a vari√°vel *mediana das idades dos casamentos*.
# 
# Assim, como esperamos, podemos escrever a priori para $\alpha$ da seguinte forma:
# 
# $$\alpha \sim Normal(0, 0.2)$$
# 
# Pois esperamos que $\alpha$ esteja pr√≥ximo de zero!
# 
# E as prioris para os $\beta$'s da seguinte forma:
# 
# $$ \beta_M \sim Normal(0, 0.5)$$
# 
# $$ \beta_A \sim Normal(0, 0.5)$$
# 
# $$ \sigma \sim Exponential(1) $$
# 
# Pois todos os valores foram padronizados, e assim esperamos que a estimativa √† priori seja pr√≥xima de zero.

# In[13]:


# =============================
#   Padronizando as vari√°veis 
# =============================
M_stdr = (df.Marriage - df.Marriage.mean())/df.Marriage.std() 
D_stdr = (df.Divorce - df.Divorce.mean())/df.Divorce.std()
A_stdr = (df.MedianAgeMarriage - df.MedianAgeMarriage.mean())/df.MedianAgeMarriage.std()


# In[19]:


# ===========================================================
#  Priori preditiva para Mediana das Idades dos Casamentos
# ===========================================================

range_A = np.linspace(A_stdr.min(), A_stdr.max())  # Range de valores da m√©dia de idade dos casamentos padronizados

qtd_amostras = 50  # Quantidade de amostras da priori preditiva

# Amostrando os valores de ùúáùëñ da priori
ùõº = np.random.normal(0, 0.2, qtd_amostras)
ùõΩ_A = np.random.normal(0, 0.5, qtd_amostras)
ùúé = np.random.exponential(1, qtd_amostras)  # Priori ùúé ~ Exp(1)


# In[20]:


# ========================================
#   Gr√°fico da Priori Preditiva para ùúáùëñ
# ========================================

ùúá = [ ùõº + ùõΩ_A * A_i for A_i in range_A ]  # Calculando ùúáùëñ = ùõº + ùõΩùê¥ * ùê¥ùëñ, para todo ùê¥ùëñ

# Plotando o gr√°fico 
plt.figure(figsize=(17, 9))

# Relembrando o DAG: A <-> D!
# Plotando as retas da posteriori para ùúáùëñ
plt.plot(range_A, ùúá, color='darkblue', linewidth=0.2)

plt.title("Priori Preditiva de $\mu_i$")
plt.xlabel("( $A$ ) M√©dia das Idades dos Casamentos (padronizadas)")
plt.ylabel("( $D$ ) Taxa de Div√≥rcio (padronizadas)")

plt.grid(ls='--', color='white', alpha=0.4)

# Ajustando os limites da visuzaliza√ß√£o do gr√°fico.
plt.ylim((-2, 2))
plt.xlim((-2, 2))

plt.show()


# Temos no gr√°fico acima a representa√ß√£o da nossa distribui√ß√£o √† priori do modelo de regress√£o linear. Sorteamos $50$ linhas dessa distribui√ß√£o. Essa √© a distribui√ß√£o  √† priori preditiva de $\mu_i$.  Ou seja, `isso √© oque nosso modelo pensa sobre o problema, antes de darmos os dados √† ele`.  
# 
# Temos que sempre fazer a simula√ß√£o da priori preditiva. S√≥ assim ser√° poss√≠vel entender uma importante caracter√≠stica presente em todos os modelo. Temos observar se a amplitude da priori faz sentido. Quando n√≥s dermos os dados para o modelo, √© necess√°rio que as estimativas do modelo resida sobre distribui√ß√£o √† priori. Caso contr√°rio, teremos uma priori ruim. Em cen√°rios mais simples, e com muitos dados, esse efeito pode at√© passar despercebido e ajustar bem. Por√©m em cen√°rios mais complexos, uma priori ruim poder√° ser desastrosa!
# 
# ```{note}
# Sempre verifique a priori.
# ```
# 
# Vamos andar pelo gr√°fico para entendermos melhor suas partes. Iniciando pela `M√©dia das Idades dos Casamentos padronizada ($A$ padronizada)` que √© uma gaussiana, centrada em $0$ e com desvio padr√£o$\frac{1}{2}$:
# 
# $$A \sim Normal(0, 0.5)$$
# 
# No gr√°fico n√≥s estamos observando todos os valores dentro do intervalo $[-2, \mbox{ } 2]$. Esse intervalo corresponde ao $2^o$ desvio padr√£o! Isso deve corresponder a grande maioria dos casos que os valores da Mediana das Idades dos Casamentos (*padronizadas*) ir√° ocorrer. 
# 
# Esse pensamento tamb√©m √© v√°lido para a vari√°vel `Taxa de Div√≥rcio ($D$ padronizada)`. Seus valores tamb√©m est√£o delimitados pelo intervalo $[-2, \mbox{ } 2]$, representando o $2^o$ desvio padr√£o.
# 
# Esse modelo nos diz √© qual a taxa de div√≥rcio padronizada,($D$), que √© uma vari√°vel *z-score*, quando j√° conhecemos os valores da M√©dia das Idades dos Casamentos ($A$), que tamb√©m est√° padronizada (outra *z-score*). Assim temos duas vari√°veis padronizadas, *z-scores*, explicando uma √† outra.
# 
# Agora, se as linhas da regress√£o linear n√£o morar no espa√ßo delimitado pela priori, ent√£o a `escolhemos uma priori ruim`! Por que isso √© imposs√≠vel! Se ap√≥s dermos os dados pra o modelo, a inclina√ß√£o da curva for maior que a delimitada pela regi√£o da priori, temos uma priori ruim.
# 
# Ainda podemos discutir se a escolha da priori ir√° governar todo o range da taxa de div√≥ricio. Provavelmente n√£o √© verdade. Em cap√≠tulos posteriores, quando falarmos sobre *overfitting* (ou em portugu√™s, teremos um sobreajuste). Isso acontece pois a priori n√£o √© apertada o suficiente para trabalharmos seguran√ßa. Nesse caso podemos pensar em prioris *flat*, que podem ser justificada cientificamente, tal como uma priori *flat* impl√≠cita nas an√°lises frequentistas. 
# 

# ## Nosso modelo

# Esse √© nosso modelo, descrito na primeira linha. Na segunda linha temos dois termos lineares, lembrando que o termo linear significa aditivo. Esses dois termos ir√° gerar um plano. A seguir, teremos nossas prioris. Um destaque para a priori do $\sigma$, de agora em diante n√£o vamos mais usar $Uniform$ mas sim a distribui√ß√£o $Exponential$, pois ela tem boas propriedades. Falaremos mais sobre isso adiante.
# 
# $$D_i \sim Normal(\mu_i, \sigma) $$
# 
# $$ \mu_i = \alpha + \beta_M M_i + \beta_A A_i $$
# 
# $$ \alpha \sim Normal(0, 0.2) $$
# 
# $$ \beta_M \sim Normal(0, 0.5) $$
# 
# $$ \beta_A \sim Normal(0, 0.5) $$
# 
# $$ \sigma \sim  Exponential(1) $$
# 
# 
# O uso da distribui√ß√£o $Exponential$ como priori para $\sigma$ tem vantagens tais como, sempre tem valores positivos, para valores maiores a sua probabilidade decresce e para defini-l√° precisamos indicar qual ser√° seu valor m√©dio.

# In[161]:


# =====================================
#  Estimando o modelo linear proposto
# =====================================

stan_model_divorce = """
    data {
        int N;
        vector[N] divorce_rate;
        vector[N] marriage_rate;
        vector[N] median_age;
    }
    
    parameters {
        real alpha;
        real beta_M;
        real beta_A;
        real<lower=0> sigma;
    }

    model{
        alpha ~ normal(0, 0.2);
        beta_M ~ normal(0,0.5);
        beta_A ~ normal(0, 0.5);
        sigma ~ exponential(1);
        
        divorce_rate ~ normal(alpha + beta_M * marriage_rate + beta_A * median_age, sigma);
    }
"""

my_data = {
    'N': len(D_stdr.values), 
    'divorce_rate': D_stdr.values,
    'marriage_rate': M_stdr.values,
    'median_age': A_stdr.values
}

posteriori_divorce = stan.build(stan_model_divorce, data=my_data)
fit_divorce = posteriori_divorce.sample(num_chains=4, num_samples=1000)

alpha = fit_divorce['alpha'].flatten()
beta_M = fit_divorce['beta_M'].flatten()
beta_A = fit_divorce['beta_A'].flatten()
sigma = fit_divorce['sigma'].flatten()


# In[162]:


def resume_posteriori(var, confidence_HPDI=0.93, rounded=2):
    """
    Return the summary of posteriori data
    """
    posteriori = []

    confi_HPDI = HPDI(var, confidence_HPDI)
    
    posteriori.append(var.mean())
    posteriori.append(var.std())
    posteriori.append(confi_HPDI[0])
    posteriori.append(confi_HPDI[1])

    return np.round(np.array([posteriori]), rounded)[0]


# In[323]:


def describe_posteriori(vars_post, confidence_HPDI=0.93, plot=True):
    
    post = []
    
    for var_ in vars_post:
        post.append(resume_posteriori(eval(var_), confidence_HPDI))
    
    hpdi_min_label = str(100 * round(1 - confidence_HPDI, 3)) + '%'
    hpdi_max_label = str(100 * round(confidence_HPDI, 3)) + '%'
        
    post = pd.DataFrame(post,
                        index=vars_post,
                        columns=['Mean', 'Std', hpdi_min_label, hpdi_max_label])
    
    if plot:
        
        plt.figure(figsize=(17, len(post) + 1))
    
        plt.title('Estimativas das Posterioris')
        
        min_axis_ = post.iloc[:, 2:4].min().min()
        max_axis_ = post.iloc[:, 2:4].max().max()

        for i in range(len(post)):
            plt.plot([min_axis_*1.5, max_axis_*1.5], [i, i], ls='--', color='gray')
            plt.plot([post.iloc[i, 2], post.iloc[i, 3]], [i, i], color='blue')
            plt.plot(post.iloc[i, 0], i, 'ko')
            plt.annotate(post.index[i], (min_axis_*1.5, i+0.2), color='blue')
            

        if min_axis_ < 0 and max_axis_ > 0:
            plt.axvline(0, ls='--', color='red', alpha=0.6)

        plt.ylim((-1, len(post)+1))
        plt.grid(ls='--', color='white', alpha=0.4)
        
        ax = plt.gca()
        ax.axes.yaxis.set_visible(False)
        
        plt.show()
    
    return post


# In[300]:


# =====================================
#  Descrevendo os dados da posteriori
# =====================================

vars_post = ['alpha', 'beta_M', 'beta_A', 'sigma']
describe_posteriori(vars_post, 0.945, plot=False)


# As informa√ß√µes da tabela acima cont√©m um resumo das posterioris estimadas. Como esperado $\alpha$ tem a m√©dia $0$. Pois tinha que ser assim conforme a constru√ß√£o do nosso modelo.
# 
# J√° a estimativa para do $\beta_M$, `taxa de casamento`, √© levemente negativa e o desvio padr√£o est√° entre $2$ a $3$ vezes a sua m√©dia. Podemos olhar para o intervalo de HPDI de $89\%$ no qual essa valores est√£o entre $-0.3$ e $0.2$. 
# 
# Talvez exista algum efeito, ou talvez n√£o tenha nenhum efeito dependendo da dire√ß√£o. N√≥s apenas n√£o sabemos muito bem o que pensar sobre essa vari√°vel, pois ela n√£o apresenta uma rela√ß√£o consistente, n√£o existe uma associa√ß√£o consistente na regress√£o multipla entre a *taxa de casamentos* e a *taxa de div√≥rcios*.
# 
# Agora, a `mediana das idades do casamentos` tem uma estimativa m√©dia de $-0.6$ com o desvio padr√£o tamb√©m de $0.16$, temos que a massa da posteriori est√° inteiramente abaixo de zero! Assim, realmente temos uma associ√ß√£o negativa entre a *media das idades dos casasmentos* e a *taxa de div√≥rcios*.
# 
# 
# Mas agora n√≥s j√° sabemos que n√£o temos um `impacto diretamente causal` entre *taxa de casamentos* e a *taxa de div√≥rcios*. Isso estava mascarado pois *idade mediana* √© uma causa comum entre as duas outras vari√°veis. 
# 

# In[324]:


vars_all = ['beta_1', 'beta_M', 'beta_2', 'beta_A'] 

#  Legendas
# ------------
# beta_2 == D ~ A   
# beta_A ==> D ~ A + M 

# beta_M ==> D ~ A + M  
# beta_1 ==> D ~ M  

describe_posteriori(vars_all, 0.945, plot=True)


# Quando observando as tabelas acima e comparando com o gr√°fico, temos que a taxa de casamentos ($\beta_M$), resultante da regress√£o multipla, tem a sua distribui√ß√£o √† posteriori, com $89\%$ de HPDI, contendo o `zero`. Portanto prov√°velmente a vari√°vel taxa de casamento ($\beta_M$) `n√£o tem impacto casual direto` na taxa de div√≥rcio. 
# 
# Ele foi mascarado por que a *mediana das idades dos casamentos* ($A$) √© uma vari√°vel que influ√™ncia tanto a *taxa de div√≥rcio* quanto a *taxa de casamentos*.
# 
# No gr√°fico acima fica claro que o modelo *beta_1*, no modelo $D ~ M$, nos diz que *taxa de casamento* tem uma associa√ß√£o positiva com a *taxa de div√≥rcio*, o que era de se esperar pelos nossos entendimentos sobre como poderia funcionar esse evento. 
# 
# Entretanto, como a regress√£o m√∫ltipla podemos ver que a posteriori atinge o zero, e assim n√£o podemos perceber que *provavelmente* n√£o existe uma associa√ß√£o entre a *taxa de casamentos* e a *taxa de div√≥rcios*.
# 
# Isso √© uma boa coisa que a regress√£o m√∫ltipla pode fazer por n√≥s! Eu acredito que `essa √© realmente a rela√ß√£o causal` que temos aqui. 
# 
# Assim, nosso *DAG* provavelmente seria dessa forma:

# In[328]:


# =======================================
#   Construindo o desenho de um DAG 
# =======================================

# Esses c√≥digos devem estar invis√≠veis no jupyter-book 

plt.figure(figsize=(17, 7))

plt.xlim(1, 2)
plt.ylim(0, 1)

size=40

#-------------------------
# DAG - Direita
#-------------------------

plt.annotate('A', (1.2, 0.8), fontsize=size)
plt.annotate('M', (1.8, 0.8), fontsize=size)
plt.annotate('D', (1.48, 0.17), fontsize=size)

plt.title('Grafo dos Div√≥rcios')

# Edge: M <---> D
#lt.annotate("", 
#             xytext=(1.79, 0.79), xy=(1.53, 0.265) ,
#             arrowprops=dict(arrowstyle="-|>,  head_width=1, head_length=1.2"))

# Edge: A <---> M
plt.annotate("", 
             xytext=(1.27, 0.82), xy=(1.77, 0.82),
             arrowprops=dict(arrowstyle="-|>,  head_width=1, head_length=1.2"))

# Edge: A <---> D
plt.annotate("", 
             xytext=(1.24, 0.77), xy=(1.48, 0.26),
             arrowprops=dict(arrowstyle="-|>,  head_width=1, head_length=1.2"))

# Remover os valores dos eixos
plt.yticks([])
plt.xticks([])

plt.show()


# Assim, como n√£o temos uma rela√ß√£o forte da *taxa de casamentos* ($M$) com a vari√°vel *taxa de div√≥rcios* ($D$), a representa√ß√£o de nosso *DAG*, ou seja, a representa√ß√£o da causalidade, poder√° ser mapeada de acordo com o gr√°fico acima. 
# 
# Mas vamos explorar um pouco melhor o significado dessa abordagem.

# ### Regress√£o M√∫ltipla

# - Uma vez que n√≥s conhecemos $A$, *mediana das idades dos casamentos*, existe apenas um pequeno valor adicional de conhecimento sobre $D$ que est√° contido em $M$, a *taxa de casamento*.
# 
# Ent√£o podemos interpretar isso da seguinte forma, ao conhecermos os valores de $A$ implica que o conhecimento de $M$ n√£o vai nos ajudar muito mais a entender $D$. Isso coincide com *DAG* acima, onde n√£o h√° rela√ß√£o causal da *taxa de casamentos* para a *taxa de div√≥rcios*.
# 
# 
# 
# - Uma vez que n√≥s conhecemos a $M$, taxa de casamentos, h√° muito valor em conhecer tamb√©m $A$, mediana das idades dos casamentos. 
# 
# Isso funciona na outra dire√ß√£o, pois a *mediana das idades dos casamentos* ($A$) √© uma `causa` comum da *taxa de casamentos* ($M$).
# 
# 
# - Se n√≥s n√£o conhecermos a *mediana das idades dos casamentos* ($A$) em algum estado, ainda √© √∫til conhecer a *taxa de casamentos* ($M$).
# 
# Saber sobre a *taxa de casamentos* √© √∫til e importante pois isso pode nos d√° informa√ß√µes adicionais. Essa informa√ß√£o vem de outra rela√ß√£o causal e n√£o da uma informa√ß√£o causal direta entre as vari√°veis.

# ```{note}
# Esse √© nosso neg√≥cio aqui. A infer√™ncia! Descobrir a diferen√ßa entre essas coisas.
# ```

# Se n√≥s apenas quisermos fazer uma previs√£o e n√£o nos importarmos com a infer√™ncia causal, a *taxa de casamento* √© √∫til e ajudar√° a prever as coisas. Mas, por outro lado, n√£o nos ajuda a fazer `intereven√ß√µes no mundo real`, porque se quisermos mudar a *taxa de div√≥rcios* ($D$) nos estado manipulando a *taxa de casamento* ($M$) n√£o teria nenhum efeito.
# 
# Simplesmente por que n√£o √© assim que as coisas funcionam. `A maquinaria de causalidade natural dos div√≥rcios n√£o apresenta essa liga√ß√£o` entre da *taxa de casamentos* para a *taxa de div√≥rcios*.
# 
# Assim, para efeito de pol√≠ticas p√∫blicas, √© necess√°rio focar nas altera√ß√µes na *mediana da idade dos casamentos* para verificar os efeitos na *taxa de div√≥rcios*.
# 
# Assim precisamos ser claros se queremos **apenas** prever as coisas. Se quisermos tamb√©m inteferir as rela√ß√µes causais, devemos fazer a previs√£o das rela√ß√µes causais para que possamos fazer as interven√ß√µes. `Uma interven√ß√£o requer um verdadeiro entendimento da causalidade do sistema`. 
# 
# ```{warning}
# Uma previs√£o n√£o tem o poder de fazer tais interven√ß√µes causais.
# ```
# 
# Isso √© o grande terror da ci√™ncia, podemos fazer previs√µes realmente boas sem entendermos nada! Lembra dos modelos *geoc√™ntricos*. `Modelo estat√≠sticos corretos n√£o s√£o suficientes para descobrir rela√ß√µes causais`, ent√£o precisamos de algo extra!

# ## Regress√£o M√∫ltipla
# 
# Parei em regress√£o m√∫ltipla 31:10
